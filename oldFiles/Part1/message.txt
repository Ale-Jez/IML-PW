. Verify Class Balance
The project requires a binary classification problem: Class 1 (Allowed) vs. Class 0 (Not Allowed). Since you likely recorded this data yourself or assembled it, your first step is checking the distribution.


What to check: Count the number of files in Class 0 and Class 1.

Why it matters: Ideally, these should be balanced. If you have 100 samples of "Allowed" and only 10 of "Not Allowed," your model might just guess "Allowed" every time.

Visualization: A simple bar chart showing the count of files per class.

2. Analyze Audio Length
The document notes that recordings can be short (1â€“3 seconds). You need to ensure consistency because Convolutional Neural Networks (CNNs) usually require fixed-size inputs.

What to check: Calculate the duration (in seconds) of every audio file.

Why it matters: If some files are 1 second and others are 10 seconds, your spectrograms will have different widths (time axis), which will break the CNN code later. You will eventually need to pad or trim them.

Visualization: A histogram of audio durations.

3. Visualize the Waveforms (Time Domain)
Before converting to spectrograms, look at the raw amplitude of the audio signals.

What to check: Plot the waveform (Amplitude vs. Time) for a few random samples from Class 1 and Class 0.

What to look for:

Silence: Are there long periods of silence at the start or end? (You will need to remove these later ).

Clipping: Does the signal hit the top/bottom limits of the graph? (This means distorted audio).

4. Visualize Spectrograms (Frequency Domain)
This is the most critical part of your EDA because the project mandates the use of spectrograms as the input for the CNN.


The Task: Convert your audio files into spectrograms (likely Mel-spectrograms) using a library like librosa.

What to check:

Visual Distinction: Plot a spectrogram for a "Class 1" person and a "Class 0" person side-by-side. Do they look distinct?


Background Noise: The document explicitly states that noise is a major issue. Look at the spectrogram in areas where no one is speaking. Is it pitch black (silence) or are there colorful specks? Colorful specks indicate background noise that you might need to address later.


Requirement: The project states you must store these plots or output them in the Jupyter Notebook to document your findings.

5. Technical "Sanity Checks"
Since you are building this for a specific hardware context (laptops with microphones), check the metadata:

Sample Rate: Ensure all your recorded files have the same sample rate (e.g., 16kHz or 44.1kHz). If they are mixed, your EDA should flag this, as you will need to resample them.

Channels: Ensure all data is mono (1 channel) or stereo (2 channels) consistently. Mono is usually preferred for voice recognition to reduce complexity.