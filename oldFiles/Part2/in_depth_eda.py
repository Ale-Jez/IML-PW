# -*- coding: utf-8 -*-
"""in-depth-eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/154nfq5H5-4w6B2algaJg83ocP9QkXAzP

# In-Depth Exploratory Data Analysis

Instruction
---
Download all the daaset files and put them in home folder of your Google Drive in the folder ML_PW.

List of Content
---

### 1. Advanced Feature Extraction & Distribution Analysis

*   MFCC (Mel Frequency Cepstral Coefficients) Analysis. Plot mean and variance distributions for Class 0 vs. Class 1.
* Spectral Features: distribution comparison for:
    * Spectral Centroid: Center of mass of the spectrum.
    * Spectral Rolloff: Frequency boundary for signal energy.
    * Zero Crossing Rate (ZCR): Rate of sign-changes (voiced vs. unvoiced speech).
* Energy Metrics: Compute Root Mean Square (RMS) energy distributions to detect volume inconsistencies.

### 2. Dimensionality Reduction & Separability

* Projecting high-dimensional feature space into 2D.
* Assessment linear vs. non-linear separability.

### 3. Quantitative Signal Quality Assessment
* Signal-to-Noise Ratio (SNR): Estimate SNR for every file. Plot distribution. Flag samples < 10dB.
* Silence Ratio: Programmatically calculate the percentage of "dead air" (e.g., < -60dB). Flag clips exceeding 20% silence.

### 4. Statistical Significance Testing
* Hypothesis Testing: Perform T-tests or Mann-Whitney U tests on:
    * Audio Duration
    * Mean Amplitude
    * Spectral Centroid
* Output: P-values confirming if Class 0 is statistically distinct from Class 1.

### 5. Correlation Analysis
* Feature Correlation Heatmap: Compute the correlation matrix of all extracted features (Duration, ZCR, Centroid, MFCCs).
* Redundancy Check: Identify features with correlation >0.9 for removal.

### 6. Outlier Detection
* Anomaly Detection: Apply Isolation Forest or Z-Score analysis on the feature set.
* Cleaning: List filenames of samples deviating significantly from the norm (potential artifacts or corrupted files).

## Summary of findings

# Library importing
"""

!pip install pandas seaborn matplotlib

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import librosa.display

import os
import random
import warnings
import numpy as np
import librosa
import h5py
import yaml
from datetime import datetime
from pydub import AudioSegment
from scipy.signal import butter, lfilter
warnings.filterwarnings("ignore", category=FutureWarning)
SUPPORTED_EXTS = (".wav", ".mp3", ".m4a", ".wma")
from google.colab import drive
drive.mount('/content/drive')

# new code for in depth eda
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

"""

#Reading the audio files
Here we iterate through all the subdirectories containing audiofiles and use label.yaml files to register them as rows in the dataframe
"""

def build_metadata_dataframe(data_root="/content/drive/MyDrive/ML_PW"):
    records = []
    # Walk through directories
    for root, dirs, files in os.walk(data_root):
        if "labels.yaml" in files:
            yaml_path = os.path.join(root, "labels.yaml")

            # Load labels file
            with open(yaml_path, 'r') as f:
                try:
                    labels_map = yaml.safe_load(f)
                    if not labels_map:
                        continue
                except yaml.YAMLError as e:
                    print(f"Error reading {yaml_path}: {e}")
                    continue

            # Process audio files in this directory
            for filename in files:
                if filename.lower().endswith(SUPPORTED_EXTS):
                    filepath = os.path.join(root, filename)
                    # Retrieve metadata from yaml
                    meta = labels_map.get(filename)
                    if meta:
                        # If it's a string format like "(M,True,Name)", parse it manually
                        if isinstance(meta, str) and meta.startswith('(') and meta.endswith(')'):
                            # 1. Remove parentheses
                            clean_meta = meta.strip("()")
                            # 2. Split by comma
                            parts = [p.strip() for p in clean_meta.split(',')]

                            # 3. Extract fields based on position
                            if len(parts) >= 3:
                                gender = parts[0]

                                is_allowed_str = parts[1]
                                label = parts[2]

                                # Convert string "True" to Boolean True
                                is_allowed = is_allowed_str == "True"
                            else:
                                print(f"Warning: malformed metadata string for {filename}: {meta}")
                                continue

                        else:
                            print(f"Warning: Unknown metadata format for {filename}: {meta}")
                            continue


                        # Get duration (fail-safe)
                        try:
                            duration = librosa.get_duration(path=filepath)
                        except Exception:
                            duration = 0

                        records.append({
                            "filepath": filepath,
                            "filename": filename,
                            "gender": gender,
                            "is_allowed": is_allowed,
                            "label": label,
                            "duration": duration
                        })
                    else:
                        # Optional: print if file exists but isn't in yaml
                        # print(f"Warning: {filename} not found in {yaml_path}")
                        pass

    df = pd.DataFrame(records)
    return df
df = build_metadata_dataframe(data_root="/content/drive/MyDrive/ML_PW")
print(df.head())

"""#Dataset
Below is the method we actually use for creating the spectrograms for our dataset with a suite of preprocessing methods. These functions serve as helpers for further development.
"""

# ==========================================================
# STEP 0: Load audio file (supports multiple formats)
# ==========================================================
def load_audio(path, sr=22050):
    """
    Load an audio file using pydub, convert it to mono, normalize bit depth,
    and resample it to the target sample rate.

    Args:
        path: Path to the audio file.
        sr: Target sample rate for resampling.

    Returns:
        samples: 1D numpy array of normalized audio samples.
        sr: The final sample rate.
    """
    audio = AudioSegment.from_file(path)

    # Convert stereo to mono if necessary
    if audio.channels > 1:
        audio = audio.set_channels(1)

    # Convert raw audio data to float32 samples
    samples = np.array(audio.get_array_of_samples()).astype(np.float32)
    samples /= (1 << (8 * audio.sample_width - 1))  # Normalize to [-1, 1] range

    # Resample to the target sample rate
    if audio.frame_rate != sr:
        samples = librosa.resample(samples, orig_sr=audio.frame_rate, target_sr=sr)

    return samples, sr


# ==========================================================
# STEP 1: Silence removal
# ==========================================================
def remove_silence(samples, sr, top_db=30):
    """
    Trim silent parts from the start and end of the signal.

    Args:
        samples: Input audio samples.
        sr: Sample rate.
        top_db: Silence threshold in decibels.

    Returns:
        Trimmed audio signal.
    """
    non_silent, _ = librosa.effects.trim(samples, top_db=top_db)
    return non_silent


# ==========================================================
# STEP 2a: Volume normalization
# ==========================================================
def volume_normalize(samples):
    """
    Normalize samples so that the maximum absolute value becomes 1.

    Args:
        samples: Input audio samples.

    Returns:
        Normalized samples.
    """
    max_val = np.max(np.abs(samples)) + 1e-9
    return samples / max_val


# ==========================================================
# STEP 2b: Filtering (band/high/low-pass)
# ==========================================================
def butter_filter(samples, sr, lowcut=None, highcut=None, order=4):
    """
    Apply a Butterworth filter (band-pass, high-pass, or low-pass).

    Args:
        samples: Input audio samples.
        sr: Sample rate.
        lowcut, highcut: Frequency cutoffs.
        order: Filter order.

    Returns:
        Filtered samples.
    """
    nyq = 0.5 * sr  # Nyquist frequency

    # Choose filter type based on which cutoffs are given
    if lowcut and highcut:
        b, a = butter(order, [lowcut / nyq, highcut / nyq], btype="band")
    elif lowcut:
        b, a = butter(order, lowcut / nyq, btype="high")
    elif highcut:
        b, a = butter(order, highcut / nyq, btype="low")
    else:
        return samples

    return lfilter(b, a, samples)


# ==========================================================
# STEP 3: Split audio into chunks
# ==========================================================
def chunk_audio(samples, sr, chunk_duration=1.0):
    """
    Split the audio into fixed-length chunks. Pads the last chunk if needed.

    Args:
        samples: Input audio samples.
        sr: Sample rate.
        chunk_duration: Duration of each chunk in seconds.

    Returns:
        List of numpy arrays representing chunks.
    """
    chunk_len = int(chunk_duration * sr)
    chunks = []

    for i in range(0, len(samples), chunk_len):
        chunk = samples[i:i + chunk_len]
        if len(chunk) < chunk_len:
            pad = np.zeros(chunk_len, dtype=samples.dtype)
            pad[:len(chunk)] = chunk
            chunk = pad
        chunks.append(chunk)

    return chunks


# ==========================================================
# STEP 4: Convert chunk to log-mel spectrogram
# ==========================================================
def to_logmel(samples, sr, n_mels=64, n_fft=2048, hop_length=512):
    """
    Convert an audio chunk into a log-mel spectrogram.

    Args:
        samples: Audio chunk.
        sr: Sample rate.
        n_mels: Number of mel bands.
        n_fft: FFT window size.
        hop_length: Hop length between frames.

    Returns:
        Log-mel spectrogram (2D numpy array).
    """
    mel = librosa.feature.melspectrogram(y=samples, sr=sr,
                                         n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)
    logmel = librosa.power_to_db(mel, ref=np.max)
    return logmel.astype(np.float32)


# ==========================================================
# Full audio â†’ list of log-mel chunks
# ==========================================================
def process_audio_to_logmels(
        path, sr=22050,
        remove_silence_flag=True,
        normalize_flag=True,
        filter_flag=False,
        lowcut=None, highcut=None,
        n_mels=64,
        chunk_duration=1.0):
    """
    Apply the entire preprocessing pipeline to one file.

    Steps:
    1. Load and resample audio
    2. Optionally remove silence
    3. Optionally normalize volume
    4. Optionally apply filtering
    5. Split into chunks
    6. Convert chunks to log-mel spectrograms
    """
    samples, sr = load_audio(path, sr)

    if remove_silence_flag:
        samples = remove_silence(samples, sr)
    if normalize_flag:
        samples = volume_normalize(samples)
    if filter_flag:
        samples = butter_filter(samples, sr, lowcut, highcut)

    chunks = chunk_audio(samples, sr, chunk_duration)
    spectrograms = [to_logmel(chunk, sr, n_mels=n_mels) for chunk in chunks]
    return spectrograms

"""# 1. Advanced Feature Extraction & Distribution Analysis



"""

N_MFCC = 13
TARGET_SR = 22050

def extract_advanced_features_row(row):
    """
    Extracts features for a single row from the metadata DataFrame.
    """
    path = row['filepath']

    try:
        # Use existing load_audio to handle formats correctly
        y, sr = load_audio(path, sr=TARGET_SR)

        # 1. MFCC Analysis (Mean & Variance)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_var = np.var(mfcc, axis=1)

        # 2. Spectral Features
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        zcr = librosa.feature.zero_crossing_rate(y)

        # 3. Energy Metrics
        rms = librosa.feature.rms(y=y)

        features = {
            'mfcc_mean': mfcc_mean,
            'mfcc_var': mfcc_var,
            'spectral_centroid': np.mean(centroid),
            'spectral_rolloff': np.mean(rolloff),
            'zero_crossing_rate': np.mean(zcr),
            'rms_energy': np.mean(rms),
            'label': 1 if row['is_allowed'] else 0  # Convert boolean to 0/1
        }
        return features

    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None

# --- Main Execution ---

print("Starting Advanced Feature Extraction...")

# Run extraction with progress bar
features_list = []
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Extracting Features"):
    feats = extract_advanced_features_row(row)
    if feats:
        features_list.append(feats)

# Create Features DataFrame
df_features = pd.DataFrame(features_list)
# Ensure 'label' is an integer type for consistent plotting
df_features['label'] = df_features['label'].astype(int)

# --- Visualization ---
sns.set_style("whitegrid")

# 1. MFCC Analysis Plot (Mean & Variance)
class_0 = df_features[df_features['label'] == 0]
class_1 = df_features[df_features['label'] == 1]

if not class_0.empty and not class_1.empty:
    # Compute aggregated means for plotting
    c0_mfcc_mean = np.mean(np.stack(class_0['mfcc_mean'].values), axis=0)
    c1_mfcc_mean = np.mean(np.stack(class_1['mfcc_mean'].values), axis=0)
    c0_mfcc_var = np.mean(np.stack(class_0['mfcc_var'].values), axis=0)
    c1_mfcc_var = np.mean(np.stack(class_1['mfcc_var'].values), axis=0)

    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # Plot MFCC Mean
    axes[0].plot(c0_mfcc_mean, label='Class 0 (Not Allowed)', marker='o', color='red')
    axes[0].plot(c1_mfcc_mean, label='Class 1 (Allowed)', marker='s', color='green')
    axes[0].set_title(f'Mean MFCC Coefficients (n={N_MFCC})')
    axes[0].set_xlabel('MFCC Coefficient Index')
    axes[0].set_ylabel('Mean Value')
    axes[0].legend()

    # Plot MFCC Variance
    axes[1].plot(c0_mfcc_var, label='Class 0 (Not Allowed)', marker='o', color='red', linestyle='--')
    axes[1].plot(c1_mfcc_var, label='Class 1 (Allowed)', marker='s', color='green', linestyle='--')
    axes[1].set_title(f'Variance of MFCC Coefficients (n={N_MFCC})')
    axes[1].set_xlabel('MFCC Coefficient Index')
    axes[1].set_ylabel('Variance')
    axes[1].legend()

    plt.tight_layout()
    plt.show()
else:
    print("Skipped MFCC plots: Class 0 or Class 1 data is missing.")

# 2. Spectral & Energy Features (Boxplots ONLY)
feature_names = ['spectral_centroid', 'spectral_rolloff', 'zero_crossing_rate', 'rms_energy']
titles = ['Spectral Centroid', 'Spectral Rolloff', 'Zero Crossing Rate', 'RMS Energy']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, col in enumerate(feature_names):
    # Standard Boxplot
    sns.boxplot(data=df_features, x='label', y=col, ax=axes[i],
                palette={'0': "red", '1': "green"}) # Changed keys to strings

    axes[i].set_title(titles[i])
    axes[i].set_xlabel('Class')
    axes[i].set_xticklabels(['Not Allowed (0)', 'Allowed (1)'])

plt.tight_layout()
plt.show()

"""# 2. Dimensionality Reduction & Separability"""

def extract_features_and_reduce_dim(df, n_mfcc=20):
    """
    Extracts MFCC features, flattens them, and applies PCA and t-SNE.
    Now includes 'Gender' for simplified visualization.
    """
    print("Extracting features for dimensionality reduction...")

    feature_list = []
    labels = []
    binary_classes = []
    genders = []  # <--- NEW: List to store gender

    for index, row in df.iterrows():
        filepath = row['filepath']
        try:
            # 1. Load and preprocess (using your existing helper functions)
            samples, sr = load_audio(filepath)
            samples = remove_silence(samples, sr)
            samples = volume_normalize(samples)

            # 2. Extract MFCCs
            mfccs = librosa.feature.mfcc(y=samples, sr=sr, n_mfcc=n_mfcc)
            mfccs_mean = np.mean(mfccs.T, axis=0)

            feature_list.append(mfccs_mean)
            labels.append(row['label'])
            binary_classes.append(row['is_allowed'])
            genders.append(row['gender']) # <--- NEW: Capture gender

        except Exception as e:
            print(f"Error processing {filepath}: {e}")

    # Convert to numpy array
    X = np.array(feature_list)

    # 3. Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 4. Apply PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    # 5. Apply t-SNE
    perp = min(30, len(feature_list) - 1) if len(feature_list) > 1 else 1
    tsne = TSNE(n_components=2, perplexity=perp, random_state=42, init='pca', learning_rate='auto')
    X_tsne = tsne.fit_transform(X_scaled)

    # Create a DataFrame for plotting
    plot_df = pd.DataFrame({
        'PCA_1': X_pca[:, 0],
        'PCA_2': X_pca[:, 1],
        'tSNE_1': X_tsne[:, 0],
        'tSNE_2': X_tsne[:, 1],
        'Label': labels,
        'Is_Allowed': binary_classes,
        'Gender': genders  # <--- NEW: Add gender to the plotting dataframe
    })

    return plot_df

def plot_dimensionality_reduction(plot_df):
    """
    Visualizes the PCA and t-SNE projections with simplified markers.
    """
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))

    # --- PCA Plot ---
    sns.scatterplot(
        ax=axes[0],
        data=plot_df,
        x='PCA_1',
        y='PCA_2',
        hue='Is_Allowed',  # Color by True/False
        style='Gender',    # Shape by Male/Female (Simplified)
        s=100,             # Marker size
        alpha=0.8,         # Transparency to see overlaps
        palette='viridis'
    )
    axes[0].set_title('PCA Projection (Linear Separability)')
    axes[0].grid(True, alpha=0.3)

    # --- t-SNE Plot ---
    sns.scatterplot(
        ax=axes[1],
        data=plot_df,
        x='tSNE_1',
        y='tSNE_2',
        hue='Is_Allowed',  # Color by True/False
        style='Gender',    # Shape by Male/Female (Simplified)
        s=100,
        alpha=0.8,
        palette='viridis'
    )
    axes[1].set_title('t-SNE Projection (Non-Linear Manifold)')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# Run the analysis
dim_red_df = extract_features_and_reduce_dim(df)
plot_dimensionality_reduction(dim_red_df)

"""
# 3. Quantitative Signal Quality Assessment"""

def calculate_signal_quality(row):
    path = row['filepath']
    try:
        # Load audio using the previously defined function
        y, sr = load_audio(path, sr=22050)

        # --- 1. Signal-to-Noise Ratio (SNR) Estimation ---
        # Heuristic: Assume lowest 10% energy frames are noise, top 10% are signal
        S = np.abs(librosa.stft(y))
        rms = librosa.feature.rms(S=S)[0]
        power = rms**2

        if len(power) == 0:
            return None

        sorted_power = np.sort(power)
        len_p = len(sorted_power)

        # Lower 10% as Noise Floor
        noise_power = np.mean(sorted_power[:int(0.1 * len_p)]) + 1e-9
        # Upper 10% as Signal Power
        signal_power = np.mean(sorted_power[int(0.9 * len_p):]) + 1e-9

        snr = 10 * np.log10(signal_power / noise_power)

        # --- 2. Silence Ratio ---
        # Calculate percentage of samples below -60dB
        # -60dB in amplitude is 10^(-60/20) = 0.001
        threshold_amp = 10**(-60/20)
        silence_count = np.sum(np.abs(y) < threshold_amp)
        silence_ratio = silence_count / len(y)

        return {
            'filename': row['filename'],
            'snr': snr,
            'silence_ratio': silence_ratio,
            'label': 1 if row['is_allowed'] else 0
        }

    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None

# --- Main Execution ---

print("Starting Quantitative Signal Quality Assessment...")

quality_data = []
# Iterate through the main dataframe created in previous steps
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Analyzing Quality"):
    res = calculate_signal_quality(row)
    if res:
        quality_data.append(res)

df_quality = pd.DataFrame(quality_data)

# --- Visualization ---
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# SNR Plot
sns.histplot(data=df_quality, x='snr', hue='label', kde=True, element="step",
             palette={0: "red", 1: "green"}, ax=axes[0])
axes[0].axvline(10, color='blue', linestyle='--', label='Threshold (10dB)')
axes[0].set_title('Distribution of SNR (Signal-to-Noise Ratio)')
axes[0].set_xlabel('SNR (dB)')
axes[0].legend()

# Silence Ratio Plot
sns.histplot(data=df_quality, x='silence_ratio', hue='label', kde=True, element="step",
             palette={0: "red", 1: "green"}, ax=axes[1])
axes[1].axvline(0.2, color='blue', linestyle='--', label='Threshold (20%)')
axes[1].set_title('Distribution of Silence Ratio')
axes[1].set_xlabel('Silence Ratio')
axes[1].legend()

plt.tight_layout()
plt.show()

# --- Flagging Samples ---
print("\n--- Quality Control Flags ---")

# Flag SNR < 10dB
low_snr = df_quality[df_quality['snr'] < 10]
print(f"Files with SNR < 10dB: {len(low_snr)}")
if not low_snr.empty:
    print(low_snr[['filename', 'snr']].head())

# Flag Silence Ratio > 20%
high_silence = df_quality[df_quality['silence_ratio'] > 0.20]
print(f"\nFiles with Silence Ratio > 20%: {len(high_silence)}")
if not high_silence.empty:
    print(high_silence[['filename', 'silence_ratio']].head())

"""# 4. Statistical Significance Testing

"""

df_stats = df_features.copy()
df_stats['duration'] = df['duration']

# Separate data for Class 0 and Class 1
class_0_data = df_stats[df_stats['label'] == 0]
class_1_data = df_stats[df_stats['label'] == 1]

features_to_test = {
    'Audio Duration': 'duration',
    'Mean Amplitude (RMS Energy)': 'rms_energy',
    'Spectral Centroid': 'spectral_centroid'
}

print("Statistical Significance Testing (Mann-Whitney U Test):\n")

for display_name, col_name in features_to_test.items():
    if not class_0_data.empty and not class_1_data.empty:
        stat, p_value = mannwhitneyu(class_0_data[col_name], class_1_data[col_name])
        print(f"Feature: {display_name}")
        print(f"  P-value: {p_value:.4f}")
        if p_value < 0.05:
            print("  Conclusion: Class 0 and Class 1 are statistically distinct.\n")
        else:
            print("  Conclusion: No significant statistical difference between Class 0 and Class 1.\n")
    else:
        print(f"Skipping {display_name}: Insufficient data for Class 0 or Class 1.\n")

"""

# 5. Correlation Analysis"""

import seaborn as sns
import matplotlib.pyplot as plt

# Select numerical features for correlation analysis
# Exclude 'label' as it's a target variable, not a feature to correlate with other features directly.
# Also exclude 'duration' as it's already included in df_stats. We are using df_corr that has duration.

features_for_correlation = [
    'duration',
    'spectral_centroid',
    'spectral_rolloff',
    'zero_crossing_rate',
    'rms_energy'
]

# Add all expanded MFCC mean and variance columns
for col in df_corr.columns:
    if col.startswith('mfcc_mean_') or col.startswith('mfcc_var_'):
        features_for_correlation.append(col)

df_correlation_subset = df_corr[features_for_correlation]

# 4. Compute the Pearson correlation matrix
correlation_matrix = df_correlation_subset.corr()

# 5. Generate a heatmap of the correlation matrix
plt.figure(figsize=(16, 14))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Feature Correlation Heatmap')
plt.show()

# 6. Identify redundant features (correlation > 0.9)
redundant_features = set()
# Iterate over the correlation matrix, but only the upper triangle to avoid duplicates and self-correlation
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.9:
            colname1 = correlation_matrix.columns[i]
            colname2 = correlation_matrix.columns[j]
            redundant_features.add(tuple(sorted((colname1, colname2)))) # Store as sorted tuple to avoid (A,B) and (B,A)

print("\nFeatures with absolute correlation > 0.9 (potential redundancy):")
if redundant_features:
    for pair in redundant_features:
        print(f"  - {pair[0]} and {pair[1]}")
else:
    print("  No features found with absolute correlation greater than 0.9.")

"""







# 6. Outlier Detection"""

from sklearn.ensemble import IsolationForest

# 1. Prepare the feature set for outlier detection
# Ensure 'df_corr' is the correct DataFrame with expanded MFCCs and duration from df_stats.
# The 'label' column should be dropped as it's not a feature for outlier detection.
features_for_outlier_detection = df_corr.drop(columns=['label'])

# The 'df_corr' already has 'mfcc_mean' and 'mfcc_var' expanded, so no list-like columns remain.
# If there were any other non-numeric or list-like columns, they should be dropped or handled.

# Ensure all columns are numeric
features_for_outlier_detection = features_for_outlier_detection.select_dtypes(include=np.number)

print(f"Shape of feature set for outlier detection: {features_for_outlier_detection.shape}")

# 2. Initialize IsolationForest model
# contamination='auto' is suitable for when you don't know the proportion of outliers
isolation_forest = IsolationForest(random_state=42, contamination='auto')

# 3. Fit the model and get anomaly predictions
# The fit_predict method returns -1 for outliers and 1 for inliers
outlier_predictions = isolation_forest.fit_predict(features_for_outlier_detection)

# 4. Add the anomaly predictions as a new column
df_corr['outlier_prediction'] = outlier_predictions

# 5. Filter df_corr to identify samples predicted as outliers (-1)
outliers_df = df_corr[df_corr['outlier_prediction'] == -1]

# 6. Retrieve the original filenames from the df DataFrame corresponding to these identified outliers
# We need the original 'df' to get the 'filename' based on the index.
# Make sure the indices align between df_corr and df.
# Assuming df_corr and df have the same original indices.

outlier_filenames = df.loc[outliers_df.index, 'filename']

print("\n--- Outlier Detection Results (Isolation Forest) ---")
print(f"Total samples identified as outliers: {len(outliers_df)}\n")

if not outliers_df.empty:
    print("Filenames of samples identified as significant outliers:")
    for idx, filename in outlier_filenames.items():
        # Get the outlier score (decision_function) for more detail
        # Lower scores indicate a higher likelihood of being an outlier
        outlier_score = isolation_forest.decision_function(features_for_outlier_detection.loc[[idx]])[0]
        print(f"- {filename} (Outlier Score: {outlier_score:.4f})")
else:
    print("No significant outliers detected.")

"""## Summarize the findings

### Statistical Significance Testing:

*   **Audio Duration**: P-value = 0.4864. No significant statistical difference between Class 0 and Class 1.
*   **Mean Amplitude (RMS Energy)**: P-value = 0.2940. No significant statistical difference between Class 0 and Class 1.
*   **Spectral Centroid**: P-value = 0.2346. No significant statistical difference between Class 0 and Class 1.

These results suggest that, based on these three features, Class 0 (Not Allowed) and Class 1 (Allowed) audio samples are not statistically distinct, which implies these features alone may not be strong discriminators for classification.

### Correlation Analysis:

*   **Highly Correlated Features (>0.9)**: 'spectral_centroid' and 'spectral_rolloff' were identified as highly correlated. This indicates redundancy between these two features, and one might be considered for removal to reduce dimensionality and avoid multicollinearity in models.

### Outlier Detection:

*   **Total Outliers Identified**: 10 samples were identified as significant outliers by the Isolation Forest model.
*   **Example Outlier Filenames**: Emma_F_False.m4a, Greta3_F_False.m4a, gatsby-ania.mp3, tiffany-Ania.m4a, sartre-ania.m4a, Recording-2.m4a, Recording-4.m4a, Recording-1.m4a, OldMan-1.mp3, Recording-3.m4a.

### Overall Summary and Insights:

The in-depth EDA has provided several key insights into the audio dataset:

1.  **Dataset Quality (SNR & Silence Ratio)**: The SNR distribution showed that all files generally have good signal quality (no files flagged below 10dB). However, a significant number of files (27) were flagged with a silence ratio exceeding 20% (e.g., Emma_F_False.m4a, Greta2_F_False.m4a). This suggests potential issues with long periods of silence within recordings, which could impact feature extraction and model performance. Preprocessing steps like silence removal are crucial.

2.  **Class Separability (PCA/t-SNE)**: The PCA and t-SNE projections of MFCC features (from the previous subtask) revealed that the two classes ('Allowed' vs. 'Not Allowed') are not clearly linearly or non-linearly separable in the 2D space. This indicates that a simple classifier might struggle to distinguish between the classes based solely on these basic acoustic features, or that more complex models and potentially additional features are needed.

3.  **Feature Redundancy**: The correlation analysis confirmed a high correlation between 'spectral_centroid' and 'spectral_rolloff'. For model building, it would be beneficial to keep only one of these features to avoid redundancy and improve model interpretability and stability.

4.  **Outliers**: The detection of 10 outliers suggests that these samples deviate significantly from the rest of the dataset's characteristics. These could be corrupted files, recordings with unusual acoustic properties, or data entry errors. Further investigation into these specific files (e.g., Emma_F_False.m4a, Greta3_F_False.m4a) is recommended. Depending on their nature, they might need cleaning, correction, or exclusion from the training set to prevent negative impacts on model training.

In conclusion, while the overall SNR is good, the presence of significant silence, feature redundancy, and a notable number of outliers highlight areas for data cleaning and preprocessing. The lack of clear statistical distinction and separability between classes also suggests that feature engineering or more sophisticated modeling techniques might be necessary to achieve good classification performance.

## Summary:

### Data Analysis Key Findings

*   **Statistical Significance**: For 'Audio Duration', 'Mean Amplitude (RMS Energy)', and 'Spectral Centroid', there was no significant statistical difference (p-values of 0.4864, 0.2940, and 0.2346 respectively) between Class 0 and Class 1, suggesting these features may not be strong discriminators.
*   **Feature Redundancy**: A high correlation (\>0.9) was found between 'spectral\_centroid' and 'spectral\_rolloff', indicating potential redundancy between these features.
*   **Outliers**: The Isolation Forest model identified 10 out of 96 samples as significant outliers, with examples including `Emma_F_False.m4a` (Outlier Score: -0.0943), `Greta3_F_False.m4a` (Outlier Score: -0.0893), and `Recording-3.m4a` (Outlier Score: -0.1199).
*   **Class Separability**: Previous PCA and t-SNE analysis of MFCC features indicated that the two classes ('Allowed' vs. 'Not Allowed') are not clearly separable in 2D space.
*   **Data Quality**: While Signal-to-Noise Ratio (SNR) was generally good across the dataset, 27 files were flagged for having a silence ratio exceeding 20%, suggesting long periods of silence.

### Insights or Next Steps

*   **Preprocessing for Silence**: Implement silence removal as a crucial preprocessing step for files with high silence ratios to improve feature extraction accuracy and reduce noise.
*   **Feature Engineering/Selection**: Consider removing one of the highly correlated features (e.g., 'spectral\_rolloff' or 'spectral\_centroid') to reduce dimensionality and improve model stability. Explore additional, more discriminative features or advanced feature engineering techniques, given the current features do not clearly separate the classes.
*   **Outlier Investigation**: Further investigate the 10 identified outlier files to understand their nature (e.g., corrupted, atypical recordings) and decide whether to clean, correct, or exclude them from the training set to prevent negative impacts on model performance.

# Task
### Plan:

*   **Re-extract Features with Silence Removal**: Modify the `qosFybOnBa1u` cell to include silence removal during the advanced feature extraction process. This will update the `df_features` DataFrame.
*   **Re-run Dimensionality Reduction**: The `tai3LrgXF0f9` cell already includes silence removal. Re-execute it to re-apply PCA and t-SNE on the newly extracted features and regenerate the plots. This will update the `dim_red_df` DataFrame.
*   **Re-run Signal Quality Assessment**: Modify the `H0JRPZf8eA4c` cell to include silence removal during SNR and silence ratio calculation. Then, re-execute it to recalculate and regenerate the plots and flagging results. This will update the `df_quality` DataFrame.
*   **Re-run Statistical Significance Testing**: Re-execute the `fa7a1110` cell to perform statistical significance tests on the updated feature set and display the new p-values.
*   **Prepare Data for Correlation Analysis**: Create and execute a new cell (conceptually `d1db505f`) to prepare `df_corr` by expanding MFCC features from the updated `df_features` and including `duration`.
*   **Re-run Correlation Analysis**: Re-execute the `69ef9278` cell to re-compute and visualize the feature correlation heatmap and identify redundant features based on the updated feature set.
*   **Re-run Outlier Detection**: Re-execute the `c96f237c` cell to re-apply the Isolation Forest model for outlier detection and list any newly identified outlier filenames.
*   **Summarize Comparison**: Compare the results of the analysis (MFCC distributions, class separability, SNR/Silence Ratio, statistical significance, feature correlations, and outlier detection) with and without silence removal. Provide insights into how silence removal impacted these findings.
*   **Final Task**: Summarize the overall impact of silence removal on the exploratory data analysis.

```python
# Modify qosFybOnBa1u cell (Advanced Feature Extraction)
# Update the extract_advanced_features_row function to include silence removal
def extract_advanced_features_row(row, remove_silence_flag=True):
    '''
    Extracts features for a single row from the metadata DataFrame.
    '''
    path = row['filepath']

    try:
        y, sr = load_audio(path, sr=TARGET_SR)

        if remove_silence_flag:
            y = remove_silence(y, sr)
            if len(y) == 0: # Handle cases where entire audio becomes silent after trimming
                # Return a dictionary with NaN for numerical features if audio is empty
                return {
                    'mfcc_mean': np.full(N_MFCC, np.nan),
                    'mfcc_var': np.full(N_MFCC, np.nan),
                    'spectral_centroid': np.nan,
                    'spectral_rolloff': np.nan,
                    'zero_crossing_rate': np.nan,
                    'rms_energy': np.nan,
                    'label': 1 if row['is_allowed'] else 0
                }

        # 1. MFCC Analysis (Mean & Variance)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_var = np.var(mfcc, axis=1)

        # 2. Spectral Features
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        zcr = librosa.feature.zero_crossing_rate(y)

        # 3. Energy Metrics
        rms = librosa.feature.rms(y=y)

        features = {
            'mfcc_mean': mfcc_mean,
            'mfcc_var': mfcc_var,
            'spectral_centroid': np.mean(centroid),
            'spectral_rolloff': np.mean(rolloff),
            'zero_crossing_rate': np.mean(zcr),
            'rms_energy': np.mean(rms),
            'label': 1 if row['is_allowed'] else 0  # Convert boolean to 0/1
        }
        return features

    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None

# --- Main Execution for qosFybOnBa1u ---

print("Starting Advanced Feature Extraction (with silence removal)...")

# Run extraction with progress bar
features_list = []
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Extracting Features"):
    feats = extract_advanced_features_row(row, remove_silence_flag=True)
    if feats:
        features_list.append(feats)

# Create Features DataFrame
df_features = pd.DataFrame(features_list)
# Drop rows that might have become entirely NaN due to silence removal
df_features.dropna(inplace=True)
# Ensure 'label' is an integer type for consistent plotting
df_features['label'] = df_features['label'].astype(int)

# --- Visualization ---
sns.set_style("whitegrid")

# 1. MFCC Analysis Plot (Mean & Variance)
class_0 = df_features[df_features['label'] == 0]
class_1 = df_features[df_features['label'] == 1]

if not class_0.empty and not class_1.empty:
    # Compute aggregated means for plotting
    c0_mfcc_mean = np.mean(np.stack(class_0['mfcc_mean'].values), axis=0)
    c1_mfcc_mean = np.mean(np.stack(class_1['mfcc_mean'].values), axis=0)
    c0_mfcc_var = np.mean(np.stack(class_0['mfcc_var'].values), axis=0)
    c1_mfcc_var = np.mean(np.stack(class_1['mfcc_var'].values), axis=0)

    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # Plot MFCC Mean
    axes[0].plot(c0_mfcc_mean, label='Class 0 (Not Allowed)', marker='o', color='red')
    axes[0].plot(c1_mfcc_mean, label='Class 1 (Allowed)', marker='s', color='green')
    axes[0].set_title(f'Mean MFCC Coefficients (n={N_MFCC})')
    axes[0].set_xlabel('MFCC Coefficient Index')
    axes[0].set_ylabel('Mean Value')
    axes[0].legend()

    # Plot MFCC Variance
    axes[1].plot(c0_mfcc_var, label='Class 0 (Not Allowed)', marker='o', color='red', linestyle='--')
    axes[1].plot(c1_mfcc_var, label='Class 1 (Allowed)', marker='s', color='green', linestyle='--')
    axes[1].set_title(f'Variance of MFCC Coefficients (n={N_MFCC})')
    axes[1].set_xlabel('MFCC Coefficient Index')
    axes[1].set_ylabel('Variance')
    axes[1].legend()

    plt.tight_layout()
    plt.show()
else:
    print("Skipped MFCC plots: Class 0 or Class 1 data is missing after silence removal.")

# 2. Spectral & Energy Features (Boxplots ONLY)
feature_names = ['spectral_centroid', 'spectral_rolloff', 'zero_crossing_rate', 'rms_energy']
titles = ['Spectral Centroid', 'Spectral Rolloff', 'Zero Crossing Rate', 'RMS Energy']

if not df_features.empty:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()

    for i, col in enumerate(feature_names):
        sns.boxplot(data=df_features, x='label', y=col, ax=axes[i],
                    palette={0: "red", 1: "green"})
        axes[i].set_title(titles[i])
        axes[i].set_xlabel('Class')
        axes[i].set_xticklabels(['Not Allowed (0)', 'Allowed (1)'])

    plt.tight_layout()
    plt.show()
else:
    print("Skipped Spectral & Energy Features plots: df_features is empty after silence removal.")
```
```python
# Re-execute tai3LrgXF0f9 (Dimensionality Reduction)
# The extract_features_and_reduce_dim function already includes remove_silence(samples, sr)
# So, no modification is needed, just re-running it will use the updated feature extraction logic
# provided in the load_audio and remove_silence functions.

# Ensure df is available and up-to-date from previous steps
if 'df' not in locals():
    print("DataFrame 'df' not found. Please ensure previous steps to load data are executed.")
else:
    print("Re-running Dimensionality Reduction (with silence removal)...")
    dim_red_df = extract_features_and_reduce_dim(df)
    plot_dimensionality_reduction(dim_red_df)
```
```python
# Modify H0JRPZf8eA4c (Signal Quality Assessment)
def calculate_signal_quality(row, remove_silence_flag=True):
    path = row['filepath']
    try:
        y, sr = load_audio(path, sr=22050)

        if remove_silence_flag:
            y_original_len = len(y)
            y = remove_silence(y, sr)
            if len(y) == 0:
                # If after removing silence, there's no audio left, consider it 100% silent
                return {
                    'filename': row['filename'],
                    'snr': -np.inf, # Indicate no signal
                    'silence_ratio': 1.0, # 100% silence
                    'label': 1 if row['is_allowed'] else 0
                }

        # --- 1. Signal-to-Noise Ratio (SNR) Estimation ---
        S = np.abs(librosa.stft(y))
        rms = librosa.feature.rms(S=S)[0]
        power = rms**2

        if len(power) == 0:
            return {
                'filename': row['filename'],
                'snr': -np.inf,
                'silence_ratio': 1.0 if remove_silence_flag else (np.sum(np.abs(y_original_len) < 10**(-60/20)) / y_original_len if y_original_len > 0 else 1.0),
                'label': 1 if row['is_allowed'] else 0
            }


        sorted_power = np.sort(power)
        len_p = len(sorted_power)

        # Lower 10% as Noise Floor
        noise_power = np.mean(sorted_power[:int(0.1 * len_p)]) + 1e-9
        # Upper 10% as Signal Power
        signal_power = np.mean(sorted_power[int(0.9 * len_p):]) + 1e-9

        snr = 10 * np.log10(signal_power / noise_power)

        # --- 2. Silence Ratio ---
        # Calculate percentage of samples below -60dB (on the *original* or *processed* signal?)
        # Since we've already removed silence for feature extraction,
        # it makes sense to calculate silence ratio on the *remaining* signal.
        # However, the previous interpretation was "dead air" in the original.
        # If silence was removed, the silence_ratio on 'y' will be very low.
        # Let's keep the original logic for calculating silence ratio on the 'y' after
        # *optional* silence removal, to see the effect.
        threshold_amp = 10**(-60/20)
        silence_count = np.sum(np.abs(y) < threshold_amp)
        silence_ratio = silence_count / len(y) if len(y) > 0 else 1.0


        return {
            'filename': row['filename'],
            'snr': snr,
            'silence_ratio': silence_ratio,
            'label': 1 if row['is_allowed'] else 0
        }

    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None

# --- Main Execution for H0JRPZf8eA4c ---

print("Starting Quantitative Signal Quality Assessment (with silence removal)...")

quality_data = []
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Analyzing Quality"):
    res = calculate_signal_quality(row, remove_silence_flag=True)
    if res:
        quality_data.append(res)

df_quality = pd.DataFrame(quality_data)

# Drop rows with NaN SNR if they occurred from entirely silent files
df_quality.replace([-np.inf], np.nan, inplace=True)
df_quality.dropna(subset=['snr'], inplace=True)


# --- Visualization ---
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# SNR Plot
sns.histplot(data=df_quality, x='snr', hue='label', kde=True, element="step",
             palette={0: "red", 1: "green"}, ax=axes[0])
axes[0].axvline(10, color='blue', linestyle='--', label='Threshold (10dB)')
axes[0].set_title('Distribution of SNR (Signal-to-Noise Ratio) after Silence Removal')
axes[0].set_xlabel('SNR (dB)')
axes[0].legend()

# Silence Ratio Plot
sns.histplot(data=df_quality, x='silence_ratio', hue='label', kde=True, element="step",
             palette={0: "red", 1: "green"}, ax=axes[1])
axes[1].axvline(0.2, color='blue', linestyle='--', label='Threshold (20%)')
axes[1].set_title('Distribution of Silence Ratio after Silence Removal')
axes[1].set_xlabel('Silence Ratio')
axes[1].legend()

plt.tight_layout()
plt.show()

# --- Flagging Samples ---
print("\n--- Quality Control Flags ---")

# Flag SNR < 10dB
low_snr = df_quality[df_quality['snr'] < 10]
print(f"Files with SNR < 10dB: {len(low_snr)}")
if not low_snr.empty:
    print(low_snr[['filename', 'snr']].head())

# Flag Silence Ratio > 20%
high_silence = df_quality[df_quality['silence_ratio'] > 0.20]
print(f"\nFiles with Silence Ratio > 20%: {len(high_silence)}")
if not high_silence.empty:
    print(high_silence[['filename', 'silence_ratio']].head())
```
```python
# Re-execute fa7a1110 (Statistical Significance Testing)
from scipy.stats import mannwhitneyu

df_stats = df_features.copy()
# df_stats['duration'] should align with the original df.
# If df_features had rows dropped, df_stats needs to reflect that for duration.
# So, we should merge or re-align duration based on the filtered df_features index.
df_stats = df_stats.merge(df[['filepath', 'duration']], on=df_features.index, how='left')
df_stats.index = df_stats['key_0']
df_stats.drop(columns=['key_0'], inplace=True)


# Separate data for Class 0 and Class 1
class_0_data = df_stats[df_stats['label'] == 0]
class_1_data = df_stats[df_stats['label'] == 1]

features_to_test = {
    'Audio Duration': 'duration',
    'Mean Amplitude (RMS Energy)': 'rms_energy',
    'Spectral Centroid': 'spectral_centroid'
}

print("Statistical Significance Testing (Mann-Whitney U Test) after Silence Removal:\n")

for display_name, col_name in features_to_test.items():
    # Drop NaNs from the specific feature before testing, as Mann-Whitney U does not handle them
    c0_series = class_0_data[col_name].dropna()
    c1_series = class_1_data[col_name].dropna()

    if not c0_series.empty and not c1_series.empty:
        stat, p_value = mannwhitneyu(c0_series, c1_series)
        print(f"Feature: {display_name}")
        print(f"  P-value: {p_value:.4f}")
        if p_value < 0.05:
            print("  Conclusion: Class 0 and Class 1 are statistically distinct.\n")
        else:
            print("  Conclusion: No significant statistical difference between Class 0 and Class 1.\n")
    else:
        print(f"Skipping {display_name}: Insufficient data for Class 0 or Class 1 after silence removal / NaN handling.\n")
```
```python
# New cell for d1db505f: Prepare Data for Correlation Analysis
print("Preparing df_corr for correlation analysis (after silence removal)...")

# Create a copy of df_features to work with
df_corr = df_features.copy()

# Add 'duration' from the original df to df_corr, aligning by index
# Ensure indices are aligned, and only include durations for the files still present in df_features
df_corr = df_corr.merge(df[['filepath', 'duration']], left_index=True, right_index=True, how='left')

# Expand mfcc_mean and mfcc_var list-like columns into individual columns
mfcc_mean_expanded = pd.DataFrame(df_corr['mfcc_mean'].tolist(), index=df_corr.index)
mfcc_var_expanded = pd.DataFrame(df_corr['mfcc_var'].tolist(), index=df_corr.index)

# Rename columns for clarity
mfcc_mean_expanded = mfcc_mean_expanded.rename(columns=lambda x: f'mfcc_mean_{x}')
mfcc_var_expanded = mfcc_var_expanded.rename(columns=lambda x: f'mfcc_var_{x}')

# Drop original list-like columns and concatenate expanded ones
df_corr = df_corr.drop(columns=['mfcc_mean', 'mfcc_var', 'filepath']) # Drop 'filepath' too as it's not a feature
df_corr = pd.concat([df_corr, mfcc_mean_expanded, mfcc_var_expanded], axis=1)

print("df_corr prepared for correlation analysis:")
print(df_corr.head())
print(f"Shape of df_corr: {df_corr.shape}")
```
```python
# Re-execute 69ef9278 (Correlation Analysis)
import seaborn as sns
import matplotlib.pyplot as plt

# Select numerical features for correlation analysis
# Exclude 'label' as it's a target variable, not a feature to correlate with other features directly.
features_for_correlation = [
    'duration',
    'spectral_centroid',
    'spectral_rolloff',
    'zero_crossing_rate',
    'rms_energy'
]

# Add all expanded MFCC mean and variance columns
for col in df_corr.columns:
    if col.startswith('mfcc_mean_') or col.startswith('mfcc_var_'):
        features_for_correlation.append(col)

# Ensure all selected features actually exist in df_corr and are numeric
features_for_correlation = [f for f in features_for_correlation if f in df_corr.columns and np.issubdtype(df_corr[f].dtype, np.number)]


df_correlation_subset = df_corr[features_for_correlation]

# Drop any NaN values that might have been introduced by silence removal before correlation calculation
df_correlation_subset.dropna(inplace=True)


# 4. Compute the Pearson correlation matrix
correlation_matrix = df_correlation_subset.corr()

# 5. Generate a heatmap of the correlation matrix
plt.figure(figsize=(16, 14))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Feature Correlation Heatmap (after Silence Removal)')
plt.show()

# 6. Identify redundant features (correlation > 0.9)
redundant_features = set()
# Iterate over the correlation matrix, but only the upper triangle to avoid duplicates and self-correlation
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.9:
            colname1 = correlation_matrix.columns[i]
            colname2 = correlation_matrix.columns[j]
            redundant_features.add(tuple(sorted((colname1, colname2)))) # Store as sorted tuple to avoid (A,B) and (B,A)

print("\nFeatures with absolute correlation > 0.9 (potential redundancy) after Silence Removal:")
if redundant_features:
    for pair in redundant_features:
        print(f"  - {pair[0]} and {pair[1]}")
else:
    print("  No features found with absolute correlation greater than 0.9.")
```
```python
# Re-execute c96f237c (Outlier Detection)
from sklearn.ensemble import IsolationForest

# 1. Prepare the feature set for outlier detection
# Ensure 'df_corr' is the correct DataFrame with expanded MFCCs and duration.
# The 'label' column should be dropped as it's not a feature for outlier detection.
features_for_outlier_detection = df_corr.drop(columns=['label'], errors='ignore') # Use errors='ignore' in case 'label' was already dropped.

# Ensure all columns are numeric and drop NaNs
features_for_outlier_detection = features_for_outlier_detection.select_dtypes(include=np.number)
features_for_outlier_detection.dropna(inplace=True) # Important to handle NaNs introduced by silence removal

print(f"Shape of feature set for outlier detection (after silence removal): {features_for_outlier_detection.shape}")

# Ensure there are enough samples to run Isolation Forest
if features_for_outlier_detection.shape[0] == 0:
    print("No data available for outlier detection after cleaning.")
else:
    # 2. Initialize IsolationForest model
    # contamination='auto' is suitable for when you don't know the proportion of outliers
    isolation_forest = IsolationForest(random_state=42, contamination='auto')

    # 3. Fit the model and get anomaly predictions
    # The fit_predict method returns -1 for outliers and 1 for inliers
    outlier_predictions = isolation_forest.fit_predict(features_for_outlier_detection)

    # 4. Add the anomaly predictions as a new column back to df_corr, ensuring index alignment
    # Create a Series with the predictions aligned to the features_for_outlier_detection index
    outlier_series = pd.Series(outlier_predictions, index=features_for_outlier_detection.index)
    df_corr['outlier_prediction'] = outlier_series


    # 5. Filter df_corr to identify samples predicted as outliers (-1)
    outliers_df = df_corr[df_corr['outlier_prediction'] == -1]

    # 6. Retrieve the original filenames from the df DataFrame corresponding to these identified outliers
    # We need the original 'df' to get the 'filename' based on the index.
    # Make sure the indices align between df_corr and df.
    # Assuming df_corr and df have the same original indices as the initial df.
    # Use df.loc to get filenames for the indices present in outliers_df
    outlier_filenames = df.loc[outliers_df.index, 'filename']


    print("\n--- Outlier Detection Results (Isolation Forest) after Silence Removal ---")
    print(f"Total samples identified as outliers: {len(outliers_df)}\n")

    if not outliers_df.empty:
        print("Filenames of samples identified as significant outliers:")
        for idx, filename in outlier_filenames.items():
            # Get the outlier score (decision_function) for more detail
            # It's crucial to select the correct row for decision_function based on the filtered index
            if idx in features_for_outlier_detection.index:
                outlier_score = isolation_forest.decision_function(features_for_outlier_detection.loc[[idx]])[0]
                print(f"- {filename} (Outlier Score: {outlier_score:.4f})")
            else:
                print(f"- {filename} (Outlier Score: N/A - data not available after filtering)")
    else:
        print("No significant outliers detected.")

```

## Re-extract Features with Silence Removal

### Subtask:
Re-execute the `qosFybOnBa1u` cell to extract advanced features, now including silence removal as a preprocessing step. This will update the `df_features` DataFrame.

**Reasoning**:
The user wants to re-execute the `qosFybOnBa1u` cell with an updated code to include silence removal. I need to modify the `extract_advanced_features_row` function to incorporate the `remove_silence` preprocessing step.
"""

N_MFCC = 13
TARGET_SR = 22050

def extract_advanced_features_row(row):
    """
    Extracts features for a single row from the metadata DataFrame.
    """
    path = row['filepath']

    try:
        # Use existing load_audio to handle formats correctly
        y, sr = load_audio(path, sr=TARGET_SR)

        # Apply silence removal
        y = remove_silence(y, sr) # ADDED LINE for silence removal

        # 1. MFCC Analysis (Mean & Variance)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_var = np.var(mfcc, axis=1)

        # 2. Spectral Features
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        zcr = librosa.feature.zero_crossing_rate(y)

        # 3. Energy Metrics
        rms = librosa.feature.rms(y=y)

        features = {
            'mfcc_mean': mfcc_mean,
            'mfcc_var': mfcc_var,
            'spectral_centroid': np.mean(centroid),
            'spectral_rolloff': np.mean(rolloff),
            'zero_crossing_rate': np.mean(zcr),
            'rms_energy': np.mean(rms),
            'label': 1 if row['is_allowed'] else 0  # Convert boolean to 0/1
        }
        return features

    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None

# --- Main Execution ---

print("Starting Advanced Feature Extraction...")

# Run extraction with progress bar
features_list = []
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Extracting Features"):
    feats = extract_advanced_features_row(row)
    if feats:
        features_list.append(feats)

# Create Features DataFrame
df_features = pd.DataFrame(features_list)
# Ensure 'label' is an integer type for consistent plotting
df_features['label'] = df_features['label'].astype(int)

# --- Visualization ---
sns.set_style("whitegrid")

# 1. MFCC Analysis Plot (Mean & Variance)
class_0 = df_features[df_features['label'] == 0]
class_1 = df_features[df_features['label'] == 1]

if not class_0.empty and not class_1.empty:
    # Compute aggregated means for plotting
    c0_mfcc_mean = np.mean(np.stack(class_0['mfcc_mean'].values), axis=0)
    c1_mfcc_mean = np.mean(np.stack(class_1['mfcc_mean'].values), axis=0)
    c0_mfcc_var = np.mean(np.stack(class_0['mfcc_var'].values), axis=0)
    c1_mfcc_var = np.mean(np.stack(class_1['mfcc_var'].values), axis=0)

    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # Plot MFCC Mean
    axes[0].plot(c0_mfcc_mean, label='Class 0 (Not Allowed)', marker='o', color='red')
    axes[0].plot(c1_mfcc_mean, label='Class 1 (Allowed)', marker='s', color='green')
    axes[0].set_title(f'Mean MFCC Coefficients (n={N_MFCC})')
    axes[0].set_xlabel('MFCC Coefficient Index')
    axes[0].set_ylabel('Mean Value')
    axes[0].legend()

    # Plot MFCC Variance
    axes[1].plot(c0_mfcc_var, label='Class 0 (Not Allowed)', marker='o', color='red', linestyle='--')
    axes[1].plot(c1_mfcc_var, label='Class 1 (Allowed)', marker='s', color='green', linestyle='--')
    axes[1].set_title(f'Variance of MFCC Coefficients (n={N_MFCC})')
    axes[1].set_xlabel('MFCC Coefficient Index')
    axes[1].set_ylabel('Variance')
    axes[1].legend()

    plt.tight_layout()
    plt.show()
else:
    print("Skipped MFCC plots: Class 0 or Class 1 data is missing.")

# 2. Spectral & Energy Features (Boxplots ONLY)
feature_names = ['spectral_centroid', 'spectral_rolloff', 'zero_crossing_rate', 'rms_energy']
titles = ['Spectral Centroid', 'Spectral Rolloff', 'Zero Crossing Rate', 'RMS Energy']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, col in enumerate(feature_names):
    # Standard Boxplot
    sns.boxplot(data=df_features, x='label', y=col, ax=axes[i],
                palette={'0': "red", '1': "green"}) # Changed keys to strings

    axes[i].set_title(titles[i])
    axes[i].set_xlabel('Class')
    axes[i].set_xticklabels(['Not Allowed (0)', 'Allowed (1)'])

plt.tight_layout()
plt.show()

"""**Reasoning**:
The previous code was not robust to cases where `remove_silence` resulted in an empty audio array, leading to errors in feature extraction. I will add a check after silence removal to return NaN values for features if the audio becomes empty, and then drop these rows from the DataFrame before plotting to ensure all subsequent calculations and visualizations work correctly.


"""

N_MFCC = 13
TARGET_SR = 22050

def extract_advanced_features_row(row):
    """
    Extracts features for a single row from the metadata DataFrame.
    """
    path = row['filepath']

    try:
        # Use existing load_audio to handle formats correctly
        y, sr = load_audio(path, sr=TARGET_SR)

        # Apply silence removal
        y = remove_silence(y, sr) # ADDED LINE for silence removal
        if len(y) == 0: # Handle cases where entire audio becomes silent after trimming
            # Return a dictionary with NaN for numerical features if audio is empty
            return {
                'mfcc_mean': np.full(N_MFCC, np.nan),
                'mfcc_var': np.full(N_MFCC, np.nan),
                'spectral_centroid': np.nan,
                'spectral_rolloff': np.nan,
                'zero_crossing_rate': np.nan,
                'rms_energy': np.nan,
                'label': 1 if row['is_allowed'] else 0
            }

        # 1. MFCC Analysis (Mean & Variance)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_var = np.var(mfcc, axis=1)

        # 2. Spectral Features
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        zcr = librosa.feature.zero_crossing_rate(y)

        # 3. Energy Metrics
        rms = librosa.feature.rms(y=y)

        features = {
            'mfcc_mean': mfcc_mean,
            'mfcc_var': mfcc_var,
            'spectral_centroid': np.mean(centroid),
            'spectral_rolloff': np.mean(rolloff),
            'zero_crossing_rate': np.mean(zcr),
            'rms_energy': np.mean(rms),
            'label': 1 if row['is_allowed'] else 0  # Convert boolean to 0/1
        }
        return features

    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None

# --- Main Execution ---

print("Starting Advanced Feature Extraction (with silence removal)...")

# Run extraction with progress bar
features_list = []
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Extracting Features"):
    feats = extract_advanced_features_row(row)
    if feats:
        features_list.append(feats)

# Create Features DataFrame
df_features = pd.DataFrame(features_list)
# Drop rows that might have become entirely NaN due to silence removal
df_features.dropna(inplace=True)
# Ensure 'label' is an integer type for consistent plotting
df_features['label'] = df_features['label'].astype(int)

# --- Visualization ---
sns.set_style("whitegrid")

# 1. MFCC Analysis Plot (Mean & Variance)
class_0 = df_features[df_features['label'] == 0]
class_1 = df_features[df_features['label'] == 1]

if not class_0.empty and not class_1.empty:
    # Compute aggregated means for plotting
    c0_mfcc_mean = np.mean(np.stack(class_0['mfcc_mean'].values), axis=0)
    c1_mfcc_mean = np.mean(np.stack(class_1['mfcc_mean'].values), axis=0)
    c0_mfcc_var = np.mean(np.stack(class_0['mfcc_var'].values), axis=0)
    c1_mfcc_var = np.mean(np.stack(class_1['mfcc_var'].values), axis=0)

    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # Plot MFCC Mean
    axes[0].plot(c0_mfcc_mean, label='Class 0 (Not Allowed)', marker='o', color='red')
    axes[0].plot(c1_mfcc_mean, label='Class 1 (Allowed)', marker='s', color='green')
    axes[0].set_title(f'Mean MFCC Coefficients (n={N_MFCC}) after Silence Removal')
    axes[0].set_xlabel('MFCC Coefficient Index')
    axes[0].set_ylabel('Mean Value')
    axes[0].legend()

    # Plot MFCC Variance
    axes[1].plot(c0_mfcc_var, label='Class 0 (Not Allowed)', marker='o', color='red', linestyle='--')
    axes[1].plot(c1_mfcc_var, label='Class 1 (Allowed)', marker='s', color='green', linestyle='--')
    axes[1].set_title(f'Variance of MFCC Coefficients (n={N_MFCC}) after Silence Removal')
    axes[1].set_xlabel('MFCC Coefficient Index')
    axes[1].set_ylabel('Variance')
    axes[1].legend()

    plt.tight_layout()
    plt.show()
else:
    print("Skipped MFCC plots: Class 0 or Class 1 data is missing after silence removal.")

# 2. Spectral & Energy Features (Boxplots ONLY)
feature_names = ['spectral_centroid', 'spectral_rolloff', 'zero_crossing_rate', 'rms_energy']
titles = ['Spectral Centroid', 'Spectral Rolloff', 'Zero Crossing Rate', 'RMS Energy']

if not df_features.empty:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()

    for i, col in enumerate(feature_names):
        # Standard Boxplot
        sns.boxplot(data=df_features, x='label', y=col, ax=axes[i],
                    palette={0: "red", 1: "green"}) # Changed keys to strings

        axes[i].set_title(titles[i] + ' after Silence Removal')
        axes[i].set_xlabel('Class')
        axes[i].set_xticklabels(['Not Allowed (0)', 'Allowed (1)'])

    plt.tight_layout()
    plt.show()
else:
    print("Skipped Spectral & Energy Features plots: df_features is empty after silence removal.")

"""**Reasoning**:
The previous `sns.boxplot` call failed because the palette dictionary expected string keys ('0', '1') while it was provided with integer keys (0, 1). I will correct the keys in the palette dictionary to be strings to match the 'label' column's type after it's been converted to `int` but is being interpreted as strings by seaborn in this context.


"""

N_MFCC = 13
TARGET_SR = 22050

def extract_advanced_features_row(row):
    """
    Extracts features for a single row from the metadata DataFrame.
    """
    path = row['filepath']

    try:
        # Use existing load_audio to handle formats correctly
        y, sr = load_audio(path, sr=TARGET_SR)

        # Apply silence removal
        y = remove_silence(y, sr) # ADDED LINE for silence removal
        if len(y) == 0: # Handle cases where entire audio becomes silent after trimming
            # Return a dictionary with NaN for numerical features if audio is empty
            return {
                'mfcc_mean': np.full(N_MFCC, np.nan),
                'mfcc_var': np.full(N_MFCC, np.nan),
                'spectral_centroid': np.nan,
                'spectral_rolloff': np.nan,
                'zero_crossing_rate': np.nan,
                'rms_energy': np.nan,
                'label': 1 if row['is_allowed'] else 0
            }

        # 1. MFCC Analysis (Mean & Variance)
mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
mfcc_mean = np.mean(mfcc, axis=1)
mfcc_var = np.var(mfcc, axis=1)

        # 2. Spectral Features
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        zcr = librosa.feature.zero_crossing_rate(y)

        # 3. Energy Metrics
        rms = librosa.feature.rms(y=y)

        features = {
            'mfcc_mean': mfcc_mean,
            'mfcc_var': mfcc_var,
            'spectral_centroid': np.mean(centroid),
            'spectral_rolloff': np.mean(rolloff),
            'zero_crossing_rate': np.mean(zcr),
            'rms_energy': np.mean(rms),
            'label': 1 if row['is_allowed'] else 0  # Convert boolean to 0/1
        }
        return features

    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None

# --- Main Execution ---

print("Starting Advanced Feature Extraction (with silence removal)...")

# Run extraction with progress bar
features_list = []
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Extracting Features"):
    feats = extract_advanced_features_row(row)
    if feats:
        features_list.append(feats)

# Create Features DataFrame
df_features = pd.DataFrame(features_list)
# Drop rows that might have become entirely NaN due to silence removal
df_features.dropna(inplace=True)
# Ensure 'label' is an integer type for consistent plotting
df_features['label'] = df_features['label'].astype(int)

# --- Visualization ---
sns.set_style("whitegrid")

# 1. MFCC Analysis Plot (Mean & Variance)
class_0 = df_features[df_features['label'] == 0]
class_1 = df_features[df_features['label'] == 1]

if not class_0.empty and not class_1.empty:
    # Compute aggregated means for plotting
    c0_mfcc_mean = np.mean(np.stack(class_0['mfcc_mean'].values), axis=0)
    c1_mfcc_mean = np.mean(np.stack(class_1['mfcc_mean'].values), axis=0)
    c0_mfcc_var = np.mean(np.stack(class_0['mfcc_var'].values), axis=0)
    c1_mfcc_var = np.mean(np.stack(class_1['mfcc_var'].values), axis=0)

    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # Plot MFCC Mean
    axes[0].plot(c0_mfcc_mean, label='Class 0 (Not Allowed)', marker='o', color='red')
    axes[0].plot(c1_mfcc_mean, label='Class 1 (Allowed)', marker='s', color='green')
    axes[0].set_title(f'Mean MFCC Coefficients (n={N_MFCC}) after Silence Removal')
    axes[0].set_xlabel('MFCC Coefficient Index')
    axes[0].set_ylabel('Mean Value')
    axes[0].legend()

    # Plot MFCC Variance
    axes[1].plot(c0_mfcc_var, label='Class 0 (Not Allowed)', marker='o', color='red', linestyle='--')
    axes[1].plot(c1_mfcc_var, label='Class 1 (Allowed)', marker='s', color='green', linestyle='--')
    axes[1].set_title(f'Variance of MFCC Coefficients (n={N_MFCC}) after Silence Removal')
    axes[1].set_xlabel('MFCC Coefficient Index')
    axes[1].set_ylabel('Variance')
    axes[1].legend()

    plt.tight_layout()
    plt.show()
else:
    print("Skipped MFCC plots: Class 0 or Class 1 data is missing after silence removal.")

# 2. Spectral & Energy Features (Boxplots ONLY)
feature_names = ['spectral_centroid', 'spectral_rolloff', 'zero_crossing_rate', 'rms_energy']
titles = ['Spectral Centroid', 'Spectral Rolloff', 'Zero Crossing Rate', 'RMS Energy']

if not df_features.empty:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()

    for i, col in enumerate(feature_names):
        # Standard Boxplot
        sns.boxplot(data=df_features, x='label', y=col, ax=axes[i],
                    palette={'0': "red", '1': "green"}) # Changed keys to strings

        axes[i].set_title(titles[i] + ' after Silence Removal')
        axes[i].set_xlabel('Class')
        axes[i].set_xticklabels(['Not Allowed (0)', 'Allowed (1)'])

    plt.tight_layout()
    plt.show()
else:
    print("Skipped Spectral & Energy Features plots: df_features is empty after silence removal.")

"""**Reasoning**:
The `IndentationError` occurred because the feature extraction code was incorrectly indented outside the `try` block. I will correct the indentation so that the feature extraction code is within the `try` block and executes after the silence removal check. Additionally, I will re-correct the `palette` keys in `sns.boxplot` back to strings, as the error indicated that seaborn expects string keys for `hue` mapping even when the column type is `int`.


"""

N_MFCC = 13
TARGET_SR = 22050

def extract_advanced_features_row(row):
    """
    Extracts features for a single row from the metadata DataFrame.
    """
    path = row['filepath']

    try:
        # Use existing load_audio to handle formats correctly
        y, sr = load_audio(path, sr=TARGET_SR)

        # Apply silence removal
        y = remove_silence(y, sr) # ADDED LINE for silence removal
        if len(y) == 0: # Handle cases where entire audio becomes silent after trimming
            # Return a dictionary with NaN for numerical features if audio is empty
            return {
                'mfcc_mean': np.full(N_MFCC, np.nan),
                'mfcc_var': np.full(N_MFCC, np.nan),
                'spectral_centroid': np.nan,
                'spectral_rolloff': np.nan,
                'zero_crossing_rate': np.nan,
                'rms_energy': np.nan,
                'label': 1 if row['is_allowed'] else 0
            }

        # 1. MFCC Analysis (Mean & Variance)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_var = np.var(mfcc, axis=1)

        # 2. Spectral Features
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        zcr = librosa.feature.zero_crossing_crossing_rate(y)

        # 3. Energy Metrics
        rms = librosa.feature.rms(y=y)

        features = {
            'mfcc_mean': mfcc_mean,
            'mfcc_var': mfcc_var,
            'spectral_centroid': np.mean(centroid),
            'spectral_rolloff': np.mean(rolloff),
            'zero_crossing_rate': np.mean(zcr),
            'rms_energy': np.mean(rms),
            'label': 1 if row['is_allowed'] else 0  # Convert boolean to 0/1
        }
        return features

    except Exception as e:
        print(f"Error processing {path}: {e}")
        return None

# --- Main Execution ---

print("Starting Advanced Feature Extraction (with silence removal)...")

# Run extraction with progress bar
features_list = []
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Extracting Features"):
    feats = extract_advanced_features_row(row)
    if feats:
        features_list.append(feats)

# Create Features DataFrame
df_features = pd.DataFrame(features_list)
# Drop rows that might have become entirely NaN due to silence removal
df_features.dropna(inplace=True)
# Ensure 'label' is an integer type for consistent plotting
df_features['label'] = df_features['label'].astype(int)

# --- Visualization ---
sns.set_style("whitegrid")

# 1. MFCC Analysis Plot (Mean & Variance)
class_0 = df_features[df_features['label'] == 0]
class_1 = df_features[df_features['label'] == 1]

if not class_0.empty and not class_1.empty:
    # Compute aggregated means for plotting
    c0_mfcc_mean = np.mean(np.stack(class_0['mfcc_mean'].values), axis=0)
    c1_mfcc_mean = np.mean(np.stack(class_1['mfcc_mean'].values), axis=0)
    c0_mfcc_var = np.mean(np.stack(class_0['mfcc_var'].values), axis=0)
    c1_mfcc_var = np.mean(np.stack(class_1['mfcc_var'].values), axis=0)

    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # Plot MFCC Mean
    axes[0].plot(c0_mfcc_mean, label='Class 0 (Not Allowed)', marker='o', color='red')
    axes[0].plot(c1_mfcc_mean, label='Class 1 (Allowed)', marker='s', color='green')
    axes[0].set_title(f'Mean MFCC Coefficients (n={N_MFCC}) after Silence Removal')
    axes[0].set_xlabel('MFCC Coefficient Index')
    axes[0].set_ylabel('Mean Value')
    axes[0].legend()

    # Plot MFCC Variance
    axes[1].plot(c0_mfcc_var, label='Class 0 (Not Allowed)', marker='o', color='red', linestyle='--')
    axes[1].plot(c1_mfcc_var, label='Class 1 (Allowed)', marker='s', color='green', linestyle='--')
    axes[1].set_title(f'Variance of MFCC Coefficients (n={N_MFCC}) after Silence Removal')
    axes[1].set_xlabel('MFCC Coefficient Index')
    axes[1].set_ylabel('Variance')
    axes[1].legend()

    plt.tight_layout()
    plt.show()
else:
    print("Skipped MFCC plots: Class 0 or Class 1 data is missing after silence removal.")

# 2. Spectral & Energy Features (Boxplots ONLY)
feature_names = ['spectral_centroid', 'spectral_rolloff', 'zero_crossing_rate', 'rms_energy']
titles = ['Spectral Centroid', 'Spectral Rolloff', 'Zero Crossing Rate', 'RMS Energy']

if not df_features.empty:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()

    for i, col in enumerate(feature_names):
        # Standard Boxplot
        sns.boxplot(data=df_features, x='label', y=col, ax=axes[i],
                    palette={'0': "red", '1': "green"}) # Corrected keys to strings

        axes[i].set_title(titles[i] + ' after Silence Removal')
        axes[i].set_xlabel('Class')
        axes[i].set_xticklabels(['Not Allowed (0)', 'Allowed (1)'])

    plt.tight_layout()
    plt.show()
else:
    print("Skipped Spectral & Energy Features plots: df_features is empty after silence removal.")