{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkuQBgk5ESoK"
      },
      "source": [
        "#Model Classifying between in-group and out-group based on short voice snippets\n",
        "\n",
        "### TBD\n",
        "- [x] general architecture\n",
        "- [x] data preprocessing pipeline (including creating the spectrograms)\n",
        "- [X] loader\n",
        "- [x] training\n",
        "- [x] validation\n",
        "- [x] logging to TensorBoard\n",
        "- [x] dataset script\n",
        "- [x] big-enough dataset\n",
        "- [ ] data augmentation pipeline to diversify the dataset\n",
        "- [ ] tuning the CNN layer and RNN layer so that it's lightweight enough to process but deep enough to generalize (try to see if it overfits at some point, if not add layers)\n",
        "- [ ] inference pipeline (described in section inference pipeline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_NSDPKfjZdJ"
      },
      "source": [
        "##DATASET\n",
        "###Creating .h5 dataset from the audio files stored in drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUeqDaBJlJT2"
      },
      "source": [
        "  labels should contain the speaker id with -1 for speakers not in the in-group\n",
        "      \n",
        "      /train/logmel      -> float (N, T, F) or (N, F, T)\n",
        "      /train/label       -> int64  (N,)           [optional]\n",
        "      /train/length      -> int64  (N,)           [optional, original T]\n",
        "    and equivalent for /val, and /test and\n",
        "      /meta/sample_rate  → attribute (scalar)\n",
        "      /meta/feature_type → \"log-mel spectrogram\"\n",
        "      /meta/description.yaml with values\n",
        "      - number_of_speakers\n",
        "      - number_of_mels\n",
        "      - frequency (a boolean True, False)\n",
        "      - dictionary of speaker ids to speaker names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p45-01eV2M8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d55920-0174-44b2-b99d-02b44a253001",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import librosa\n",
        "import h5py\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "from pydub import AudioSegment\n",
        "from scipy.signal import butter, lfilter\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "SUPPORTED_EXTS = (\".wav\", \".mp3\", \".m4a\", \".wma\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRn3OrROizSV"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# INFERENCE-COMPATIBLE PREPROCESSING CLASS\n",
        "# ==========================================================\n",
        "class AudioPreprocessor:\n",
        "    \"\"\"Lightweight preprocessing pipeline.\"\"\"\n",
        "    def __init__(self, sr=16000, n_mels=64, n_fft=2048, hop_length=512,\n",
        "                 chunk_duration=1.0, remove_silence=True, normalize=True,\n",
        "                 lowcut=None, highcut=None, filter_order=4):\n",
        "        self.sr = sr\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.chunk_duration = chunk_duration\n",
        "        self.remove_silence = remove_silence\n",
        "        self.normalize = normalize\n",
        "        self.lowcut = lowcut\n",
        "        self.highcut = highcut\n",
        "        self.filter_order = filter_order\n",
        "\n",
        "    def load_audio(self, path):\n",
        "        \"\"\"Load and resample audio.\"\"\"\n",
        "        audio = AudioSegment.from_file(path)\n",
        "        if audio.channels > 1:\n",
        "            audio = audio.set_channels(1)\n",
        "\n",
        "        samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
        "        samples /= (1 << (8 * audio.sample_width - 1))\n",
        "\n",
        "        if audio.frame_rate != self.sr:\n",
        "            samples = librosa.resample(samples, orig_sr=audio.frame_rate, target_sr=self.sr)\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def trim_silence(self, samples, top_db=30):\n",
        "        \"\"\"Remove silence from audio.\"\"\"\n",
        "        non_silent, _ = librosa.effects.trim(samples, top_db=top_db)\n",
        "        return non_silent\n",
        "\n",
        "    def normalize_volume(self, samples):\n",
        "        \"\"\"Normalize audio volume.\"\"\"\n",
        "        max_val = np.max(np.abs(samples)) + 1e-9\n",
        "        return samples / max_val\n",
        "\n",
        "    def apply_filter(self, samples):\n",
        "        \"\"\"Apply Butterworth filter.\"\"\"\n",
        "        if not self.lowcut and not self.highcut:\n",
        "            return samples\n",
        "\n",
        "        nyq = 0.5 * self.sr\n",
        "        if self.lowcut and self.highcut:\n",
        "            b, a = butter(self.filter_order, [self.lowcut / nyq, self.highcut / nyq], btype=\"band\")\n",
        "        elif self.lowcut:\n",
        "            b, a = butter(self.filter_order, self.lowcut / nyq, btype=\"high\")\n",
        "        else:\n",
        "            b, a = butter(self.filter_order, self.highcut / nyq, btype=\"low\")\n",
        "\n",
        "        return lfilter(b, a, samples)\n",
        "\n",
        "    def chunk_audio(self, samples):\n",
        "        \"\"\"Split audio into fixed-length chunks.\"\"\"\n",
        "        chunk_len = int(self.chunk_duration * self.sr)\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(samples), chunk_len):\n",
        "            chunk = samples[i:i + chunk_len]\n",
        "            if len(chunk) < chunk_len:\n",
        "                pad = np.zeros(chunk_len, dtype=samples.dtype)\n",
        "                pad[:len(chunk)] = chunk\n",
        "                chunk = pad\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def to_logmel(self, samples):\n",
        "        \"\"\"Convert audio to log-mel spectrogram.\"\"\"\n",
        "        mel = librosa.feature.melspectrogram(\n",
        "            y=samples, sr=self.sr, n_mels=self.n_mels,\n",
        "            n_fft=self.n_fft, hop_length=self.hop_length\n",
        "        )\n",
        "        logmel = librosa.power_to_db(mel, ref=np.max)\n",
        "        return logmel.astype(np.float32)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return configuration dict.\"\"\"\n",
        "        return {\n",
        "            \"sr\": self.sr,\n",
        "            \"n_mels\": self.n_mels,\n",
        "            \"n_fft\": self.n_fft,\n",
        "            \"hop_length\": self.hop_length,\n",
        "            \"chunk_duration\": self.chunk_duration,\n",
        "            \"remove_silence\": self.remove_silence,\n",
        "            \"normalize\": self.normalize,\n",
        "            \"lowcut\": self.lowcut,\n",
        "            \"highcut\": self.highcut,\n",
        "            \"filter_order\": self.filter_order\n",
        "        }\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# AUGMENTATION FUNCTIONS\n",
        "# ==========================================================\n",
        "def add_ambient_noise(samples, noise_factor=0.005):\n",
        "    \"\"\"Add Gaussian noise.\"\"\"\n",
        "    noise = np.random.randn(len(samples)) * noise_factor\n",
        "    return samples + noise\n",
        "\n",
        "\n",
        "def speed_perturbation(samples, sr, speed_factor=1.0):\n",
        "    \"\"\"Apply time stretching.\"\"\"\n",
        "    return librosa.effects.time_stretch(samples, rate=speed_factor)\n",
        "\n",
        "\n",
        "def spectral_augmentation(logmel, freq_mask_param=10, time_mask_param=20, n_masks=2):\n",
        "    \"\"\"Apply SpecAugment-style masking.\"\"\"\n",
        "    augmented = logmel.copy()\n",
        "    n_mels, n_frames = augmented.shape\n",
        "\n",
        "    for _ in range(n_masks):\n",
        "        f = np.random.randint(0, min(freq_mask_param, n_mels))\n",
        "        f0 = np.random.randint(0, max(1, n_mels - f))\n",
        "        augmented[f0:f0+f, :] = augmented.mean()\n",
        "\n",
        "    for _ in range(n_masks):\n",
        "        t = np.random.randint(0, min(time_mask_param, n_frames))\n",
        "        t0 = np.random.randint(0, max(1, n_frames - t))\n",
        "        augmented[:, t0:t0+t] = augmented.mean()\n",
        "\n",
        "    return augmented\n",
        "\n",
        "\n",
        "def vocal_tract_length_perturbation(samples, sr, alpha=1.0):\n",
        "    \"\"\"Apply VTLP via frequency warping.\"\"\"\n",
        "    D = librosa.stft(samples)\n",
        "    n_fft = (D.shape[0] - 1) * 2\n",
        "    freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
        "    warped_freqs = freqs * alpha\n",
        "\n",
        "    D_warped = np.zeros_like(D)\n",
        "    for i, wf in enumerate(warped_freqs):\n",
        "        if wf < freqs[-1]:\n",
        "            idx = np.searchsorted(freqs, wf)\n",
        "            if idx < len(freqs) - 1:\n",
        "                D_warped[i] = D[idx]\n",
        "\n",
        "    return librosa.istft(D_warped, length=len(samples))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# METADATA PARSING\n",
        "# ==========================================================\n",
        "def parse_labels_yaml(yaml_path):\n",
        "    if not os.path.exists(yaml_path):\n",
        "        return {}\n",
        "\n",
        "    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
        "        data = yaml.safe_load(f)\n",
        "\n",
        "    if not data:\n",
        "        return {}\n",
        "\n",
        "    parsed = {}\n",
        "    for filename, value in data.items():\n",
        "        if isinstance(value, (list, tuple)) and len(value) >= 3:\n",
        "            gender, in_group, speaker_name = value[0], value[1], value[2]\n",
        "            parsed[filename] = {\n",
        "                \"gender\": gender,\n",
        "                \"in_group\": in_group,\n",
        "                \"speaker_name\": speaker_name\n",
        "            }\n",
        "        elif isinstance(value, dict):\n",
        "            parsed[filename] = {\n",
        "                \"gender\": value.get(\"gender\", \"Unknown\"),\n",
        "                \"in_group\": value.get(\"in_group\", False),\n",
        "                \"speaker_name\": value.get(\"speaker_name\", os.path.splitext(filename)[0])\n",
        "            }\n",
        "\n",
        "    return parsed\n",
        "\n",
        "\n",
        "def get_speaker_label_mapping(data_root=\"/content/drive/MyDrive/ML_PW/Recordings\"):\n",
        "    \"\"\"\n",
        "    Create a mapping from speaker_name to unique label.\n",
        "    \"\"\"\n",
        "    speaker_to_label = {}\n",
        "    label_counter = 0\n",
        "\n",
        "    # First pass: collect all unique speaker names\n",
        "    for folder in os.listdir(data_root):\n",
        "        folder_path = os.path.join(data_root, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        labels_yaml_path = os.path.join(folder_path, \"labels.yaml\")\n",
        "        file_metadata = parse_labels_yaml(labels_yaml_path)\n",
        "\n",
        "        for filename, info in file_metadata.items():\n",
        "            speaker_name = info[\"speaker_name\"]\n",
        "            if speaker_name not in speaker_to_label:\n",
        "                speaker_to_label[speaker_name] = label_counter\n",
        "                label_counter += 1\n",
        "\n",
        "    # Second pass: add folder names as speakers\n",
        "    for folder in os.listdir(data_root):\n",
        "        folder_path = os.path.join(data_root, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            if folder not in speaker_to_label:\n",
        "                speaker_to_label[folder] = label_counter\n",
        "                label_counter += 1\n",
        "\n",
        "    label_to_speaker = {v: k for k, v in speaker_to_label.items()}\n",
        "\n",
        "    return speaker_to_label, label_to_speaker\n",
        "\n",
        "\n",
        "def process_audio_file(path, preprocessor, speaker_label, speaker_info,\n",
        "                      with_noise=False, with_speed=False, with_vtlp=False,\n",
        "                      with_spectral=False):\n",
        "    \"\"\"Process a single audio file with augmentations.\"\"\"\n",
        "    samples = preprocessor.load_audio(path)\n",
        "    if preprocessor.remove_silence:\n",
        "        samples = preprocessor.trim_silence(samples)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    processed = samples.copy()\n",
        "    if preprocessor.normalize:\n",
        "        processed = preprocessor.normalize_volume(processed)\n",
        "    if preprocessor.lowcut or preprocessor.highcut:\n",
        "        processed = preprocessor.apply_filter(processed)\n",
        "\n",
        "    # Original samples\n",
        "    chunks = preprocessor.chunk_audio(processed)\n",
        "    specs = [preprocessor.to_logmel(c) for c in chunks]\n",
        "\n",
        "    for spec in specs:\n",
        "        all_results.append((spec, \"original\", False, speaker_label, speaker_info, path))\n",
        "\n",
        "    # Augmentations\n",
        "    if with_noise:\n",
        "        noise_factor = np.random.uniform(0.002, 0.008)\n",
        "        noisy = add_ambient_noise(processed, noise_factor)\n",
        "        chunks_noisy = preprocessor.chunk_audio(noisy)\n",
        "        specs_noisy = [preprocessor.to_logmel(c) for c in chunks_noisy]\n",
        "        if with_spectral:\n",
        "            specs_noisy = [spectral_augmentation(s) for s in specs_noisy]\n",
        "        for spec in specs_noisy:\n",
        "            all_results.append((spec, f\"noise_{noise_factor:.4f}\", False, speaker_label, speaker_info, path))\n",
        "\n",
        "    if with_speed:\n",
        "        for speed_factor in [0.9, 1.1]:\n",
        "            try:\n",
        "                sped = speed_perturbation(processed, preprocessor.sr, speed_factor)\n",
        "                chunks_sped = preprocessor.chunk_audio(sped)\n",
        "                specs_sped = [preprocessor.to_logmel(c) for c in chunks_sped]\n",
        "                if with_spectral:\n",
        "                    specs_sped = [spectral_augmentation(s) for s in specs_sped]\n",
        "                for spec in specs_sped:\n",
        "                    all_results.append((spec, f\"speed_{speed_factor:.1f}\", False, speaker_label, speaker_info, path))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    if with_vtlp:\n",
        "        for alpha in [0.9, 1.1]:\n",
        "            try:\n",
        "                vtlped = vocal_tract_length_perturbation(processed, preprocessor.sr, alpha)\n",
        "                chunks_vtlp = preprocessor.chunk_audio(vtlped)\n",
        "                specs_vtlp = [preprocessor.to_logmel(c) for c in chunks_vtlp]\n",
        "                if with_spectral:\n",
        "                    specs_vtlp = [spectral_augmentation(s) for s in specs_vtlp]\n",
        "                for spec in specs_vtlp:\n",
        "                    all_results.append((spec, f\"vtlp_{alpha:.1f}\", True, speaker_label, speaker_info, path))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return all_results"
      ],
      "metadata": {
        "id": "zmyRu0t5vjy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mya5lMl5i9Tt"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# PROCESS ENTIRE DATASET\n",
        "# ==========================================================\n",
        "def process_dataset_with_speaker_names(data_root=\"/content/drive/MyDrive/ML_PW/Recordings\", preprocessor=None,\n",
        "                                      with_noise=False, with_speed=False,\n",
        "                                      with_vtlp=False, with_spectral=False):\n",
        "    \"\"\"Process entire dataset using speaker names from YAML files.\"\"\"\n",
        "    if preprocessor is None:\n",
        "        raise ValueError(\"Preprocessor is required\")\n",
        "\n",
        "    speaker_to_label, label_to_speaker = get_speaker_label_mapping(data_root)\n",
        "\n",
        "    all_specs = []\n",
        "    all_labels = []\n",
        "    all_aug_types = []\n",
        "    all_is_vtlp = []\n",
        "    all_speaker_info = []\n",
        "    all_file_paths = []\n",
        "\n",
        "    print(f\"\\nSpeaker mapping:\")\n",
        "    for speaker_name, label in speaker_to_label.items():\n",
        "        print(f\"  {speaker_name} -> label {label}\")\n",
        "\n",
        "    next_vtlp_label = 10000\n",
        "    vtlp_mapping = {}\n",
        "\n",
        "    for folder in os.listdir(data_root):\n",
        "        folder_path = os.path.join(data_root, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nProcessing folder: {folder}\")\n",
        "\n",
        "        labels_yaml_path = os.path.join(folder_path, \"labels.yaml\")\n",
        "        file_metadata = parse_labels_yaml(labels_yaml_path)\n",
        "\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if not filename.lower().endswith(SUPPORTED_EXTS):\n",
        "                continue\n",
        "\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            if filename in file_metadata:\n",
        "                speaker_info = file_metadata[filename]\n",
        "                speaker_name = speaker_info[\"speaker_name\"]\n",
        "            else:\n",
        "                speaker_name = folder\n",
        "                speaker_info = {\n",
        "                    \"gender\": \"Unknown\",\n",
        "                    \"in_group\": True,\n",
        "                    \"speaker_name\": speaker_name\n",
        "                }\n",
        "\n",
        "            speaker_label = speaker_to_label.get(speaker_name, len(speaker_to_label))\n",
        "\n",
        "            print(f\"  {filename}: speaker '{speaker_name}' -> label {speaker_label}\")\n",
        "\n",
        "            try:\n",
        "                results = process_audio_file(\n",
        "                    file_path, preprocessor, speaker_label, speaker_info,\n",
        "                    with_noise=with_noise, with_speed=with_speed,\n",
        "                    with_vtlp=with_vtlp, with_spectral=with_spectral\n",
        "                )\n",
        "\n",
        "                for spec, aug_type, is_vtlp, spk_label, spk_info, fpath in results:\n",
        "                    if is_vtlp:\n",
        "                        key = (spk_label, aug_type)\n",
        "                        if key not in vtlp_mapping:\n",
        "                            vtlp_mapping[key] = next_vtlp_label\n",
        "                            next_vtlp_label += 1\n",
        "                        final_label = vtlp_mapping[key]\n",
        "                    else:\n",
        "                        final_label = spk_label\n",
        "\n",
        "                    all_specs.append(spec)\n",
        "                    all_labels.append(final_label)\n",
        "                    all_aug_types.append(aug_type)\n",
        "                    all_is_vtlp.append(is_vtlp)\n",
        "                    all_speaker_info.append(spk_info)\n",
        "                    all_file_paths.append(fpath)\n",
        "\n",
        "                print(f\"    {len(results)} chunks\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error: {e}\")\n",
        "\n",
        "    for (base_label, aug_type), vtlp_label in vtlp_mapping.items():\n",
        "        base_speaker = label_to_speaker[base_label]\n",
        "        speaker_to_label[f\"{base_speaker}_{aug_type}\"] = vtlp_label\n",
        "        label_to_speaker[vtlp_label] = f\"{base_speaker}_{aug_type}\"\n",
        "\n",
        "    print(f\"\\nDone. Total spectrograms: {len(all_specs)}\")\n",
        "    print(f\"Base speakers: {len(speaker_to_label) - len(vtlp_mapping)}\")\n",
        "    print(f\"VTLP speakers: {len(vtlp_mapping)}\")\n",
        "    print(f\"Total speakers: {len(speaker_to_label)}\")\n",
        "\n",
        "    return (np.array(all_specs, dtype=object),\n",
        "            np.array(all_labels),\n",
        "            all_aug_types,\n",
        "            np.array(all_is_vtlp),\n",
        "            all_speaker_info,\n",
        "            all_file_paths,\n",
        "            speaker_to_label,\n",
        "            label_to_speaker,\n",
        "            vtlp_mapping)\n",
        "\n",
        "\n",
        "def create_80_10_10_splits_by_file(spectrograms, labels, aug_types, is_vtlp,\n",
        "                                   speaker_info_list, file_paths, random_seed=42):\n",
        "    \"\"\"Create 80-10-10 splits at FILE level.\"\"\"\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    file_to_data = defaultdict(lambda: {\n",
        "        \"specs\": [], \"labels\": [], \"aug_types\": [],\n",
        "        \"is_vtlp\": [], \"speaker_info\": []\n",
        "    })\n",
        "\n",
        "    for i, file_path in enumerate(file_paths):\n",
        "        file_to_data[file_path][\"specs\"].append(spectrograms[i])\n",
        "        file_to_data[file_path][\"labels\"].append(labels[i])\n",
        "        file_to_data[file_path][\"aug_types\"].append(aug_types[i])\n",
        "        file_to_data[file_path][\"is_vtlp\"].append(is_vtlp[i])\n",
        "        file_to_data[file_path][\"speaker_info\"].append(speaker_info_list[i])\n",
        "\n",
        "    all_files = list(file_to_data.keys())\n",
        "    random.shuffle(all_files)\n",
        "\n",
        "    n_files = len(all_files)\n",
        "    n_train = int(0.8 * n_files)\n",
        "    n_test = int(0.1 * n_files)\n",
        "\n",
        "    train_files = all_files[:n_train]\n",
        "    test_files = all_files[n_train:n_train + n_test]\n",
        "    val_files = all_files[n_train + n_test:]\n",
        "\n",
        "    print(f\"\\nFile-level 80-10-10 split:\")\n",
        "    print(f\"  Total files: {n_files}\")\n",
        "    print(f\"  Train: {len(train_files)} files ({len(train_files)/n_files*100:.1f}%)\")\n",
        "    print(f\"  Test:  {len(test_files)} files ({len(test_files)/n_files*100:.1f}%)\")\n",
        "    print(f\"  Val:   {len(val_files)} files ({len(val_files)/n_files*100:.1f}%)\")\n",
        "\n",
        "    splits = {\n",
        "        \"train\": {\"specs\": [], \"labels\": [], \"aug_types\": [],\n",
        "                 \"is_vtlp\": [], \"speaker_info\": [], \"file_paths\": []},\n",
        "        \"val\": {\"specs\": [], \"labels\": [], \"aug_types\": [],\n",
        "               \"is_vtlp\": [], \"speaker_info\": [], \"file_paths\": []},\n",
        "        \"test\": {\"specs\": [], \"labels\": [], \"aug_types\": [],\n",
        "                \"is_vtlp\": [], \"speaker_info\": [], \"file_paths\": []}\n",
        "    }\n",
        "\n",
        "    file_to_split = {}\n",
        "    for f in train_files:\n",
        "        file_to_split[f] = \"train\"\n",
        "    for f in val_files:\n",
        "        file_to_split[f] = \"val\"\n",
        "    for f in test_files:\n",
        "        file_to_split[f] = \"test\"\n",
        "\n",
        "    for file_path, data in file_to_data.items():\n",
        "        split_name = file_to_split[file_path]\n",
        "        splits[split_name][\"specs\"].extend(data[\"specs\"])\n",
        "        splits[split_name][\"labels\"].extend(data[\"labels\"])\n",
        "        splits[split_name][\"aug_types\"].extend(data[\"aug_types\"])\n",
        "        splits[split_name][\"is_vtlp\"].extend(data[\"is_vtlp\"])\n",
        "        splits[split_name][\"speaker_info\"].extend(data[\"speaker_info\"])\n",
        "        splits[split_name][\"file_paths\"].extend([file_path] * len(data[\"specs\"]))\n",
        "\n",
        "    for split_name in splits:\n",
        "        if len(splits[split_name][\"specs\"]) > 0:\n",
        "            splits[split_name][\"specs\"] = np.array(splits[split_name][\"specs\"], dtype=object)\n",
        "            splits[split_name][\"labels\"] = np.array(splits[split_name][\"labels\"])\n",
        "            splits[split_name][\"is_vtlp\"] = np.array(splits[split_name][\"is_vtlp\"])\n",
        "\n",
        "    print(f\"\\nSample distribution:\")\n",
        "    total = sum(len(splits[s][\"specs\"]) for s in splits if len(splits[s][\"specs\"]) > 0)\n",
        "    for split_name in [\"train\", \"val\", \"test\"]:\n",
        "        if len(splits[split_name][\"specs\"]) > 0:\n",
        "            n = len(splits[split_name][\"specs\"])\n",
        "            print(f\"  {split_name}: {n} samples ({n/total*100:.1f}%)\")\n",
        "\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWsngaJHn2y7"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# SAVE TO HDF5\n",
        "# ==========================================================\n",
        "def save_h5_dataset(splits, speaker_to_label, label_to_speaker, vtlp_mapping,\n",
        "                   preprocessor, out_dir=\"/content/drive/MyDrive/ML_PW/outputs\"):\n",
        "    \"\"\"Save dataset to HDF5.\"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    date_tag = datetime.now().strftime(\"%d-%m-%y\")\n",
        "\n",
        "    aug_types_present = set()\n",
        "    for split_data in splits.values():\n",
        "        for aug in split_data[\"aug_types\"]:\n",
        "            if \"noise\" in aug:\n",
        "                aug_types_present.add(\"noise\")\n",
        "            if \"speed\" in aug:\n",
        "                aug_types_present.add(\"speed\")\n",
        "            if \"vtlp\" in aug:\n",
        "                aug_types_present.add(\"vtlp\")\n",
        "\n",
        "    aug_tag = \"_\".join(sorted(aug_types_present)) if aug_types_present else \"no_aug\"\n",
        "    out_name = f\"logmels_{aug_tag}_{date_tag}.h5\"\n",
        "    out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "    base_labels = [l for l in speaker_to_label.values() if l < 10000]\n",
        "    vtlp_labels = [l for l in speaker_to_label.values() if l >= 10000]\n",
        "\n",
        "    with h5py.File(out_path, 'w') as f:\n",
        "        for split_name, split_data in splits.items():\n",
        "            if len(split_data[\"specs\"]) == 0:\n",
        "                continue\n",
        "\n",
        "            specs = split_data[\"specs\"]\n",
        "            labels = split_data[\"labels\"]\n",
        "            aug_types = split_data[\"aug_types\"]\n",
        "            file_paths = split_data[\"file_paths\"]\n",
        "\n",
        "            n_mels = specs[0].shape[0]\n",
        "            max_T = max(s.shape[1] for s in specs)\n",
        "\n",
        "            arr = np.zeros((len(specs), n_mels, max_T), dtype=np.float32)\n",
        "            lengths = np.zeros(len(specs), dtype=np.int64)\n",
        "\n",
        "            for i, s in enumerate(specs):\n",
        "                T = s.shape[1]\n",
        "                arr[i, :, :T] = s\n",
        "                lengths[i] = T\n",
        "\n",
        "            grp = f.create_group(split_name)\n",
        "            grp.create_dataset(\"logmel\", data=arr, compression=\"gzip\")\n",
        "            grp.create_dataset(\"label\", data=np.array(labels, dtype=np.int64), compression=\"gzip\")\n",
        "            grp.create_dataset(\"length\", data=lengths, compression=\"gzip\")\n",
        "\n",
        "            dt = h5py.string_dtype(encoding='utf-8')\n",
        "            aug_array = np.array([str(a).encode('utf-8') if not isinstance(a, bytes) else a\n",
        "                                 for a in aug_types], dtype=dt)\n",
        "            grp.create_dataset(\"augmentation\", data=aug_array, compression=\"gzip\")\n",
        "\n",
        "            file_array = np.array([str(p).encode('utf-8') if not isinstance(p, bytes) else p\n",
        "                                  for p in file_paths], dtype=dt)\n",
        "            grp.create_dataset(\"file_path\", data=file_array, compression=\"gzip\")\n",
        "\n",
        "        meta = f.create_group(\"meta\")\n",
        "        cfg = preprocessor.get_config()\n",
        "\n",
        "        split_stats = {}\n",
        "        for split_name in splits:\n",
        "            if len(splits[split_name][\"specs\"]) > 0:\n",
        "                split_labels = splits[split_name][\"labels\"]\n",
        "                unique_labels = set(split_labels)\n",
        "                base_count = len([l for l in unique_labels if l < 10000])\n",
        "                vtlp_count = len([l for l in unique_labels if l >= 10000])\n",
        "\n",
        "                split_stats[split_name] = {\n",
        "                    \"num_samples\": len(split_labels),\n",
        "                    \"num_speakers\": len(unique_labels),\n",
        "                    \"num_base_speakers\": base_count,\n",
        "                    \"num_vtlp_speakers\": vtlp_count\n",
        "                }\n",
        "\n",
        "        meta.attrs.update({\n",
        "            \"sample_rate\": cfg[\"sr\"],\n",
        "            \"feature_type\": \"log-mel spectrogram\",\n",
        "            \"split_strategy\": \"80-10-10 by file\",\n",
        "            \"num_base_speakers\": len(base_labels),\n",
        "            \"num_vtlp_speakers\": len(vtlp_labels),\n",
        "            \"total_speakers\": len(speaker_to_label)\n",
        "        })\n",
        "\n",
        "        file_desc = {\n",
        "            \"dataset_name\": out_name,\n",
        "            \"preprocessing_config\": cfg,\n",
        "            \"split_strategy\": \"80-10-10 by file (random split)\",\n",
        "            \"split_statistics\": split_stats,\n",
        "            \"total_base_speakers\": len(base_labels),\n",
        "            \"total_vtlp_speakers\": len(vtlp_labels),\n",
        "            \"total_speakers\": len(speaker_to_label),\n",
        "            \"vtlp_speaker_id_start\": 10000,\n",
        "            \"note\": \"File-level split ensures no temporal overlap\"\n",
        "        }\n",
        "\n",
        "        speaker_mapping = {}\n",
        "        for speaker_name, label in speaker_to_label.items():\n",
        "            speaker_mapping[speaker_name] = {\n",
        "                \"label\": int(label),\n",
        "                \"is_vtlp\": label >= 10000\n",
        "            }\n",
        "\n",
        "        vtlp_mapping_clean = {}\n",
        "        for (base_label, aug_type), vtlp_label in vtlp_mapping.items():\n",
        "            base_speaker = label_to_speaker.get(base_label, f\"Unknown_{base_label}\")\n",
        "            vtlp_mapping_clean[f\"{base_speaker}_{aug_type}\"] = {\n",
        "                \"vtlp_label\": int(vtlp_label),\n",
        "                \"base_label\": int(base_label),\n",
        "                \"augmentation\": aug_type\n",
        "            }\n",
        "\n",
        "        meta.create_dataset(\"file_description.yaml\",\n",
        "                           data=np.bytes_(yaml.safe_dump(file_desc, default_flow_style=False)))\n",
        "        meta.create_dataset(\"speaker_mapping.yaml\",\n",
        "                           data=np.bytes_(yaml.safe_dump({\"speakers\": speaker_mapping})))\n",
        "        meta.create_dataset(\"vtlp_mapping.yaml\",\n",
        "                           data=np.bytes_(yaml.safe_dump({\"vtlp_speakers\": vtlp_mapping_clean})))\n",
        "        meta.create_dataset(\"split_statistics.json\",\n",
        "                           data=np.bytes_(json.dumps(split_stats, indent=2).encode('utf-8')))\n",
        "\n",
        "    print(f\"\\nSaved dataset → {out_path}\")\n",
        "    print(f\"  Base speakers: {len(base_labels)}\")\n",
        "    print(f\"  VTLP speakers: {len(vtlp_labels)}\")\n",
        "    print(f\"  Total speakers: {len(speaker_to_label)}\")\n",
        "\n",
        "    return out_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCct-a9pjAO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd29451-b92c-4d82-b671-25c0a4aecc35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Speaker mapping:\n",
            "  Piotr -> label 0\n",
            "  Aleksander -> label 1\n",
            "  Mantas -> label 2\n",
            "  Rafał -> label 3\n",
            "  michał -> label 4\n",
            "\n",
            "Processing folder: Piotr\n",
            "  Emma_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    78 chunks\n",
            "  Greta2_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1186 chunks\n",
            "  Obama2_M_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1565 chunks\n",
            "  Anne_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1566 chunks\n",
            "  Greta4_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1560 chunks\n",
            "  Greta1_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1385 chunks\n",
            "  Julian_M_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1475 chunks\n",
            "  Greta3_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    120 chunks\n",
            "  Piotr_M_True.m4a: speaker 'Piotr' -> label 0\n",
            "    7254 chunks\n",
            "  marzena-2.mp3: speaker 'Piotr' -> label 0\n",
            "    1114 chunks\n",
            "  Dominika_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1831 chunks\n",
            "  Lara_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1523 chunks\n",
            "  Natalia_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1469 chunks\n",
            "  Obama_M_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1558 chunks\n",
            "  Marzena_F_False.m4a: speaker 'Piotr' -> label 0\n",
            "    1150 chunks\n",
            "\n",
            "Processing folder: Aleksander\n",
            "  jfk-2-2.m4a: speaker 'Aleksander' -> label 1\n",
            "    1649 chunks\n",
            "  jfk-2-1.m4a: speaker 'Aleksander' -> label 1\n",
            "    1523 chunks\n",
            "  oppenheimer.wav: speaker 'Aleksander' -> label 1\n",
            "    325 chunks\n",
            "  jfk-1.wav: speaker 'Aleksander' -> label 1\n",
            "    1265 chunks\n",
            "  oversimplified-1.wav: speaker 'Aleksander' -> label 1\n",
            "    813 chunks\n",
            "  reagan-2.wav: speaker 'Aleksander' -> label 1\n",
            "    1403 chunks\n",
            "  churchill-1.wav: speaker 'Aleksander' -> label 1\n",
            "    1643 chunks\n",
            "  fdr.wav: speaker 'Aleksander' -> label 1\n",
            "    1734 chunks\n",
            "  reagan-1.wav: speaker 'Aleksander' -> label 1\n",
            "    1672 chunks\n",
            "  oversimplified-2.wav: speaker 'Aleksander' -> label 1\n",
            "    1150 chunks\n",
            "  queen-1.wav: speaker 'Aleksander' -> label 1\n",
            "    1138 chunks\n",
            "  grian3.wav: speaker 'Aleksander' -> label 1\n",
            "    964 chunks\n",
            "  jfk-3.wav: speaker 'Aleksander' -> label 1\n",
            "    2120 chunks\n",
            "  grian-1.wav: speaker 'Aleksander' -> label 1\n",
            "    174 chunks\n",
            "  gatsby-ania.mp3: speaker 'Aleksander' -> label 1\n",
            "    3131 chunks\n",
            "  adi.wav: speaker 'Aleksander' -> label 1\n",
            "    2282 chunks\n",
            "  pati-glacier.mp3: speaker 'Aleksander' -> label 1\n",
            "    1481 chunks\n",
            "  invocation-Pan-Tadeusz.mp3: speaker 'Aleksander' -> label 1\n",
            "    434 chunks\n",
            "  tiffany-Ania.m4a: speaker 'Aleksander' -> label 1\n",
            "    729 chunks\n",
            "  queen-3.wav: speaker 'Aleksander' -> label 1\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# MAIN EXECUTION\n",
        "# ==========================================================\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = AudioPreprocessor(\n",
        "    sr=16000,\n",
        "    n_mels=64,\n",
        "    n_fft=2048,\n",
        "    hop_length=512,\n",
        "    chunk_duration=1.0,\n",
        "    remove_silence=True,\n",
        "    normalize=True,\n",
        "    lowcut=None,\n",
        "    highcut=None\n",
        ")\n",
        "\n",
        "# AUGMENTATION CONTROLS - Set these booleans\n",
        "WITH_NOISE = True\n",
        "WITH_SPEED = True\n",
        "WITH_VTLP = True\n",
        "WITH_SPECTRAL = True\n",
        "\n",
        "# Process dataset\n",
        "specs, labels, aug_types, is_vtlp, spk_info, file_paths, \\\n",
        "speaker_to_label, label_to_speaker, vtlp_mapping = process_dataset_with_speaker_names(\n",
        "    data_root=\"/content/drive/MyDrive/ML_PW/Recordings\",\n",
        "    preprocessor=preprocessor,\n",
        "    with_noise=WITH_NOISE,\n",
        "    with_speed=WITH_SPEED,\n",
        "    with_vtlp=WITH_VTLP,\n",
        "    with_spectral=WITH_SPECTRAL\n",
        ")\n",
        "\n",
        "# Create 80-10-10 splits by file\n",
        "print(\"\\nCreating 80-10-10 splits by file...\")\n",
        "splits = create_80_10_10_splits_by_file(\n",
        "    specs, labels, aug_types, is_vtlp, spk_info, file_paths, random_seed=42\n",
        ")\n",
        "\n",
        "# Save to HDF5\n",
        "if len(specs) > 0:\n",
        "    save_h5_dataset(\n",
        "        splits, speaker_to_label, label_to_speaker, vtlp_mapping,\n",
        "        preprocessor, out_dir=\"/content/drive/MyDrive/ML_PW/outputs\"\n",
        "    )\n",
        "    print(\"\\n✅ Dataset generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1tLqlbIDyL1"
      },
      "source": [
        "## MODEL ARCHITECTURE\n",
        "### Simple Outline\n",
        "- CNN block which pools only the frequency preserving time-axis\n",
        "- RNN block which process time-axis data\n",
        "- if training then AAM Softmax, else just outputting\n",
        "- all of that is contained in the SpeakerClassifier Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k002pomUtMX3"
      },
      "outputs": [],
      "source": [
        "!pip install  scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyngjqAKwInM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# mount Drive once per session\n",
        "from google.colab import drive  # type: ignore\n",
        "\n",
        "drive.mount('/content/drive/',force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels: int, reduction: int = 16):\n",
        "        super().__init__()\n",
        "        hidden = max(channels // reduction, 4)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, hidden, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, T, F]\n",
        "        B, C, T, F = x.shape\n",
        "        # Global average pooling over time + frequency\n",
        "        s = x.mean(dim=(2, 3))              # [B, C]\n",
        "        w = self.fc(s)                      # [B, C]\n",
        "        w = w.view(B, C, 1, 1)              # [B, C, 1, 1]\n",
        "        return x * w"
      ],
      "metadata": {
        "id": "gMkBQsVper0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CntNUNmmyBVX"
      },
      "outputs": [],
      "source": [
        "#input is log-mel spectrogram with shape [B, 1, T, F]\n",
        "# B - batch size, 1 - dimension (assuming mono-audio),\n",
        "# T - time frames, F - frequency bins\n",
        "class Backbone(nn.Module):\n",
        "  \"\"\"\n",
        "  Backbone of model, unifies entry CNN block with RNN block.\n",
        "  \"\"\"\n",
        "  def __init__(self, no_mels,embed_dim,rnn_hidden,rnn_layers,bidir):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    self.cnn_block = nn.Sequential(\n",
        "\n",
        "      nn.Conv2d(1, 32, 3, padding=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      SEBlock(32, reduction=8),\n",
        "      nn.MaxPool2d(kernel_size=(1, 2)),\n",
        "\n",
        "\n",
        "      nn.Conv2d(32, 64, 3, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      SEBlock(64, reduction=8),\n",
        "      nn.MaxPool2d(kernel_size=(1, 2)),\n",
        "\n",
        "\n",
        "      nn.Conv2d(64, 128, 3, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      SEBlock(128, reduction=8),\n",
        "      nn.MaxPool2d(kernel_size=(1, 2)),\n",
        "\n",
        "    )\n",
        "\n",
        "    self.rnn_hidden = rnn_hidden #for easier reconfiguration in the future\n",
        "    self.rnn = nn.GRU( #gated recurrent unit (RNN)\n",
        "        input_size = 128 * (no_mels // 8), # 8 = 2^3 because 3 pooling by 2\n",
        "        hidden_size=self.rnn_hidden,\n",
        "        num_layers=rnn_layers,\n",
        "        bidirectional=bidir, #if true the GRU learns in both directions: forward direction → from past to future, backward → from future to past.\n",
        "        batch_first=True,\n",
        "        dropout=0.2\n",
        "        )\n",
        "\n",
        "    #EMBEDDING HEAD\n",
        "    out_dim = (2 if bidir else 1) * rnn_hidden\n",
        "\n",
        "    self.rnn_ln = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    #simple attention for ASP(Attentive Statistics Pooling)\n",
        "    #essentially ignores boring, uninteresting moments\n",
        "    self.att = nn.Sequential(\n",
        "              nn.Linear((2 if bidir else 1)*rnn_hidden, 128),\n",
        "              nn.Tanh(),\n",
        "              nn.Linear(128, 1)\n",
        "          )\n",
        "\n",
        "    self.proj = nn.Sequential(\n",
        "        nn.Linear(out_dim*2, 256),\n",
        "        nn.BatchNorm1d(256), nn.ReLU(),\n",
        "        nn.Linear(256, embed_dim)   #embedding to given embedding dimension\n",
        "              )\n",
        "\n",
        "\n",
        "  def forward(self,x, lengths: torch.Tensor | None = None, mc_dropout: bool | None = None):\n",
        "    if mc_dropout is None:\n",
        "        mc_dropout = self.training\n",
        "    #PIPELINE\n",
        "    h = self.cnn_block(x) #process through CNN\n",
        "    if mc_dropout:\n",
        "      h = F.dropout(h, p=0.3, training=True) #monte carlo droupout applied\n",
        "    #shape after cnn_block [B, C, T, F (pooled)]\n",
        "    B,C,T,Fp = h.shape\n",
        "    h = h.permute(0,2,1,3).contiguous().view(B,T,C*Fp) # reshape for time sequence analysis\n",
        "\n",
        "\n",
        "    if lengths is not None:\n",
        "      packed = nn.utils.rnn.pack_padded_sequence(\n",
        "          h,\n",
        "          lengths.cpu(),\n",
        "          batch_first=True,\n",
        "          enforce_sorted=False,\n",
        "          )\n",
        "      packed_out, _ = self.rnn(packed)\n",
        "\n",
        "      rnn_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "      Tmax = rnn_out.size(1)\n",
        "      mask = (torch.arange(Tmax, device=rnn_out.device).unsqueeze(0).expand(B, Tmax)) < lengths.unsqueeze(1)\n",
        "\n",
        "    else:\n",
        "       rnn_out, _ = self.rnn(h) #process reshaped cnn output through rnn,  outputs [B, T, H×2]\n",
        "       mask = torch.ones(rnn_out.size(0), rnn_out.size(1),\n",
        "                        dtype=torch.bool, device=rnn_out.device)\n",
        "\n",
        "    if mc_dropout:\n",
        "            rnn_out = F.dropout(rnn_out, p=0.3, training=True) #monte carlo droupout applied\n",
        "\n",
        "    rnn_out = self.rnn_ln(rnn_out)\n",
        "\n",
        "    #STATISTICS\n",
        "    a = self.att(rnn_out).squeeze(-1) #attention weights over time, dim: BxTx1\n",
        "    a = a.masked_fill(~mask, float('-inf'))\n",
        "    w = torch.softmax(a,dim=1).unsqueeze(-1)  # dim: BxTx1\n",
        "\n",
        "    mean = torch.sum(w*rnn_out,dim=1) # dim: BxH\n",
        "    var  = torch.sum(w * (rnn_out - mean.unsqueeze(1))**2, dim=1)\n",
        "    std  = torch.sqrt(var + 1e-5)\n",
        "    stats = torch.cat([mean, std], 1)\n",
        "\n",
        "    if mc_dropout:\n",
        "            stats = F.dropout(stats, p=0.3, training=True)\n",
        "\n",
        "    z = self.proj(stats)                  # B×emb_dim\n",
        "    z = nn.functional.normalize(z, p=2, dim=1)\n",
        "\n",
        "    return z\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBzKzria2vN4"
      },
      "outputs": [],
      "source": [
        "#AAMSoftmax head to enhance class identification using margins, https://medium.com/@zhaomin.chen/additive-margin-softmax-loss-3c78e37b08ed\n",
        "#essentially tighter intra-class clusters and larger inter-class gaps\n",
        "#used only for training, discarded at inference time\n",
        "class AAMSoftmax(nn.Module):\n",
        "  \"\"\"\n",
        "  AAMSoftmax head to enhance class identification at training time.\n",
        "  Discarded at inference time.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_features, out_features, s=30.0, m=0.20):\n",
        "    super().__init__()\n",
        "    self.s = s\n",
        "    self.m = m\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
        "    nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "\n",
        "  def forward(self,emb,labels):\n",
        "    W = F.normalize(self.weight, dim=1) #normalize weight vectors, embedding is already normalized\n",
        "\n",
        "    cos_theta = emb @ W.T #get the cosine similarities using matrix product\n",
        "\n",
        "    #increase the margins\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    theta = torch.acos(cos_theta.clamp(-1+1e-7, 1-1e-7))\n",
        "    target_logits = torch.cos(theta + self.m)\n",
        "\n",
        "    #one-hot encoding, substituting only for true speaker\n",
        "    one_hot = F.one_hot(labels, num_classes=W.size(0)).float()\n",
        "    output = cos_theta * (1 - one_hot) + target_logits * one_hot\n",
        "\n",
        "    return output * self.s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AEPyeAxAqR0"
      },
      "outputs": [],
      "source": [
        "class SpeakerClassifier(nn.Module):\n",
        "  \"\"\"\n",
        "  Class unifying backbone with AAMSoftmax head. Used for training and inference.\n",
        "  \"\"\"\n",
        "  def __init__(self,backbone,num_speakers,aamsm_scaler,aamsm_margin):\n",
        "    super().__init__()\n",
        "    self.backbone=backbone\n",
        "    self.aamsm = AAMSoftmax(backbone.proj[-1].out_features,num_speakers,aamsm_scaler,aamsm_margin)\n",
        "    self._inference_prepared = False\n",
        "    self.score_alpha = nn.Parameter(torch.tensor(1.0))  # scale\n",
        "    self.score_beta  = nn.Parameter(torch.tensor(0.0))  # bias\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x,labels=None,lengths=None):\n",
        "    emb = self.backbone(x,lengths=lengths,mc_dropout=None )\n",
        "    if labels is not None:\n",
        "      logits = self.aamsm(emb,labels)\n",
        "      return logits,emb\n",
        "    else:\n",
        "      return emb\n",
        "\n",
        "  def eval(self):\n",
        "    super().eval()   #keeps original behaviour\n",
        "\n",
        "    # Build bank for inference if it's not already built\n",
        "    if not self._inference_prepared:\n",
        "        with torch.no_grad():\n",
        "            self.bank = F.normalize(self.aamsm.weight, dim=1)\n",
        "        self._inference_prepared = True\n",
        "\n",
        "    return self\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed(self, x, lengths=None):\n",
        "    z = self.backbone(x, lengths=lengths)\n",
        "    return F.normalize(z, dim=1)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def mc_embed(self, x, lengths=None, n_samples: int = 10):\n",
        "    \"\"\"\n",
        "    Monte Carlo dropout embeddings.\n",
        "    Returns:\n",
        "        embs: [n_samples, B, D]\n",
        "    \"\"\"\n",
        "    embs = []\n",
        "    for _ in range(n_samples):\n",
        "        z = self.backbone(x, lengths=lengths, mc_dropout=True)\n",
        "        z = F.normalize(z, dim=1)\n",
        "        embs.append(z)\n",
        "    return torch.stack(embs, dim=0)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def build_bank_from_aam(self):\n",
        "    return F.normalize(self.aamsm.weight, dim=1)\n",
        "\n",
        "  def set_default_inference_threshold(self, threshold):\n",
        "    self.inference_threshold = threshold\n",
        "\n",
        "  # @torch.no_grad()\n",
        "  # def prepare_for_inference(self, threshold: float = 0.35):\n",
        "  #   \"\"\"\n",
        "  #   Build a speaker bank from AAMSoftmax weights and put model into eval mode.\n",
        "  #   After this, .infer(...) can be used directly with no calibration/enrollment.\n",
        "  #   \"\"\"\n",
        "  #   self.eval()\n",
        "  #   bank = self.build_bank_from_aam()          # [S, D]\n",
        "  #   # store as buffer so it moves with model.to(device)\n",
        "  #   self.bank = bank\n",
        "  #   self.inference_threshold = threshold\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def verify_any(self, x, lengths=None, *, threshold=None, bank=None, return_index=False):\n",
        "    \"\"\"\n",
        "    Compare input embeddings against a speaker bank (default: self.bank built from AAM weights).\n",
        "\n",
        "    x:       [B, 1, T, F]\n",
        "    lengths: [B] (optional, time lengths)\n",
        "    Returns:\n",
        "        decisions: [B] bool\n",
        "        scores:    [B] float (max cosine similarity)\n",
        "        (optional) indices: [B] long (argmax speaker index)\n",
        "    \"\"\"\n",
        "    if bank is None:\n",
        "        if self.bank is None:\n",
        "            raise RuntimeError('The bank has not been built before inference. Make sure the model has been set to eval mode!')\n",
        "        else:\n",
        "            bank = self.bank\n",
        "    bank = F.normalize(bank, dim=1)\n",
        "\n",
        "    probe = self.embed(x, lengths=lengths)      # [B, D]\n",
        "    sims = probe @ bank.T                       # [B, S]\n",
        "    scores, idx = sims.max(dim=1)               # max over speakers\n",
        "\n",
        "    scores = self.score_alpha * scores + self.score_beta\n",
        "\n",
        "    thr = self.inference_threshold if threshold is None else threshold\n",
        "    decisions = scores >= thr\n",
        "\n",
        "    if return_index:\n",
        "        return decisions, scores, idx\n",
        "    return decisions, scores\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def mc_verify_any(self, x, lengths=None, n_samples: int = 10,\n",
        "                      threshold=None, bank=None, return_index=False):\n",
        "    \"\"\"\n",
        "    MC Dropout version of verify_any.\n",
        "\n",
        "    Returns:\n",
        "        decisions: [B] bool (based on mean score)\n",
        "        mean_scores: [B] float\n",
        "        var_scores: [B] float (uncertainty)\n",
        "        (optional) pred_idx: [B] long from mean scores\n",
        "    \"\"\"\n",
        "    self.eval()\n",
        "    if bank is None:\n",
        "      if self.bank is None:\n",
        "          raise RuntimeError('The bank has not been built before inference. Make sure the model has been set to eval mode!')\n",
        "      else:\n",
        "          bank = self.bank\n",
        "    bank = F.normalize(bank, dim=1)\n",
        "\n",
        "    embs = self.mc_embed(x, lengths=lengths)      # [B, D]\n",
        "    sims = torch.einsum(\"kbd,sd->kbs\", embs, bank)  # [K, B, S]\n",
        "    mean_sims = sims.mean(dim=0)  # [B, S]\n",
        "    var_sims  = sims.var(dim=0)   # [B, S]\n",
        "    mean_scores, pred_idx = mean_sims.max(dim=1)  # [B]\n",
        "\n",
        "    mean_scores = self.score_alpha * mean_scores + self.score_beta\n",
        "    score_var = var_sims.gather(1, pred_idx.view(-1, 1)).squeeze(1)  # [B]\n",
        "\n",
        "    thr = self.inference_threshold if threshold is None else threshold\n",
        "    decisions = mean_scores >= thr\n",
        "\n",
        "    if return_index:\n",
        "        return decisions, mean_scores, score_var, pred_idx\n",
        "    return decisions, mean_scores, score_var\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def infer(self, x, lengths=None, threshold=None):\n",
        "    \"\"\"\n",
        "    High-level inference method.\n",
        "\n",
        "    Input:\n",
        "        x:       tensor [B, 1, T, F] (log-mel spectrograms)\n",
        "        lengths: tensor [B] with valid time lengths (optional)\n",
        "    Output:\n",
        "        pred_ids:   [B] long  -- predicted speaker index (0..num_speakers-1)\n",
        "        scores:     [B] float -- cosine similarity to predicted prototype\n",
        "        decisions:  [B] bool  -- whether score >= threshold\n",
        "    \"\"\"\n",
        "    decisions, scores, pred_ids = self.verify_any(\n",
        "        x, lengths=lengths, threshold=threshold, return_index=True\n",
        "    )\n",
        "    return pred_ids, scores, decisions\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test(self, loaders):\n",
        "    self.eval()\n",
        "    device = self.device if self.device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.to(device)\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in loaders['test']:\n",
        "            X, y, lengths = batch                   # no need for unpack()\n",
        "\n",
        "            X = X.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            lengths = lengths.to(device, non_blocking=True)\n",
        "\n",
        "            for i in range(X.size(0)):              # per-item verify\n",
        "                x_single = X[i:i+1]                 # keep batch dim [1,1,T,F]\n",
        "                l_single = lengths[i:i+1]           # keep batch dim [1]\n",
        "                pred_ids, scores, decisions = self.infer(\n",
        "                    x=x_single,\n",
        "                    lengths=l_single,\n",
        "                    threshold=0.7\n",
        "                )\n",
        "                results.append((\n",
        "                    int(pred_ids.item()),         # True/False\n",
        "                    float(scores.item()),            # similarity\n",
        "\n",
        "                    int(y[i].item())               # true label for this item\n",
        "                ))\n",
        "    print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7g3NzNMELYT"
      },
      "source": [
        "##Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfFiGAdFLjxV"
      },
      "outputs": [],
      "source": [
        "#utilities for saving locally and in colab\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def prepare_save_dir(checkpoint_dir: str, use_drive: bool=False, drive_dir: str=\"/content/drive/MyDrive/ML_PW/spk_checkpoints\"):\n",
        "    save_root = checkpoint_dir\n",
        "    i = 1\n",
        "    PATH  = save_root+\"/train\"+str(i)\n",
        "    while os.path.exists(PATH):\n",
        "      i+=1\n",
        "      PATH = save_root+\"/train\"+str(i)\n",
        "    os.makedirs(PATH)\n",
        "\n",
        "    if use_drive and in_colab():\n",
        "\n",
        "        save_root = drive_dir\n",
        "    os.makedirs(save_root, exist_ok=True)\n",
        "    return PATH\n",
        "\n",
        "def save_checkpoint(model: torch.nn.Module, path: str):\n",
        "    # minimal, robust save (state_dict only)\n",
        "    torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj0MwJaRDlkf"
      },
      "outputs": [],
      "source": [
        "def unpack(batch):\n",
        "    # Supports both (X,y) and (X,y,lengths)\n",
        "    if len(batch) == 3:\n",
        "        X, y, lengths = batch\n",
        "    else:\n",
        "        X, y = batch\n",
        "        lengths = None\n",
        "    return X, y, lengths\n",
        "\n",
        "#TRAINING AND VALIDATION funcs\n",
        "def train_one_epoch(model, loader, optimizer, scheduler, writer, epoch, device,global_step,loss_fn,scaler,use_amp=True):\n",
        "  model.train()\n",
        "  total_loss, total_correct, total_samples = 0.0, 0, 0\n",
        "  predicted_labels, ground_truth_labels = [], []\n",
        "  total_time = batch_count = 0\n",
        "\n",
        "  for batch in loader:\n",
        "    batch_count+=1\n",
        "    batch_time = time.perf_counter()\n",
        "    X, y, lengths = unpack(batch)\n",
        "\n",
        "    X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "    if lengths is not None:\n",
        "        lengths = lengths.to(device, non_blocking=True)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    with autocast(enabled=(use_amp and device.type == \"cuda\")):\n",
        "      logits, emb = model(X, y, lengths=lengths)\n",
        "      loss = loss_fn(logits, y)\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "\n",
        "    pred = logits.argmax(1)\n",
        "\n",
        "    predicted_labels.append(pred.cpu().detach().numpy())\n",
        "    ground_truth_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "    correct = (pred==y).sum().item()\n",
        "    total_correct += correct\n",
        "    total_samples += y.size(0)\n",
        "    total_loss += loss.item() * y.size(0)\n",
        "\n",
        "    batch_time = time.perf_counter() - batch_time\n",
        "    total_time+=batch_time\n",
        "\n",
        "\n",
        "    # TensorBoard logging\n",
        "    writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
        "    writer.add_scalar(\"train/acc\", correct / y.size(0), global_step)\n",
        "    global_step += 1\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  y_true = np.concatenate(ground_truth_labels)\n",
        "  y_pred = np.concatenate(predicted_labels)\n",
        "  print(f\"Training batches took {total_time/batch_count*1000}ms per batch.\")\n",
        "  #Per-class:\n",
        "  labels = np.unique(y_true)\n",
        "  prec, rec, f1, supp = precision_recall_fscore_support(\n",
        "      y_true, y_pred,\n",
        "      beta=1.0,\n",
        "      average=None,\n",
        "      labels=labels,\n",
        "      zero_division=0,\n",
        "  )\n",
        "  #Macro-average instead:\n",
        "  prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "      y_true, y_pred, beta=1.0, average=\"macro\", zero_division=0\n",
        "  )\n",
        "\n",
        "  for label, p, r, f, s in zip(labels, prec, rec, f1, supp):\n",
        "      writer.add_scalar(f\"train/precision_class_{label}\", p, epoch)\n",
        "      writer.add_scalar(f\"train/recall_class_{label}\",    r, epoch)\n",
        "      writer.add_scalar(f\"train/f1_class_{label}\",        f, epoch)\n",
        "      writer.add_scalar(f\"train/support_class_{label}\",   s, epoch)\n",
        "  writer.add_scalar(\"train/precision_macro\", prec_macro, epoch)\n",
        "  writer.add_scalar(\"train/recall_macro\",    rec_macro,  epoch)\n",
        "  writer.add_scalar(\"train/f1_macro\",        f1_macro,   epoch)\n",
        "\n",
        "  avg_loss = total_loss / total_samples\n",
        "  avg_acc = total_correct / total_samples\n",
        "\n",
        "  print(f\"{\"Train\":<20}| Epoch {epoch:03d} | loss {avg_loss:.4f} | acc {avg_acc:.4f} | precision {prec_macro:.3f} | f1-score {f1_macro:.3f} |\")\n",
        "  return global_step,avg_loss, avg_acc\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, writer, epoch, device,loss_fn):\n",
        "  model.eval()\n",
        "  start_time = time.perf_counter()\n",
        "  total_loss, total_correct, total_samples = 0.0, 0, 0\n",
        "  predicted_labels, ground_truth_labels = [], []\n",
        "\n",
        "  start_time = time.perf_counter()\n",
        "  total_time = batch_count = 0\n",
        "  for batch in loader:\n",
        "    batch_count+=1\n",
        "    batch_time = time.perf_counter()\n",
        "    Xv, yv, lengths = unpack(batch)\n",
        "    Xv, yv = Xv.to(device, non_blocking=True), yv.to(device, non_blocking=True)\n",
        "    if lengths is not None:\n",
        "        lengths = lengths.to(device, non_blocking=True)\n",
        "    logits, _ = model(Xv, yv, lengths=lengths)\n",
        "    loss = loss_fn(logits, yv)\n",
        "\n",
        "    pred = logits.argmax(1)\n",
        "\n",
        "    predicted_labels.append(pred.cpu().detach().numpy())\n",
        "    ground_truth_labels.append(yv.cpu().detach().numpy())\n",
        "    batch_time = time.perf_counter() - batch_time\n",
        "    total_time += batch_time\n",
        "    #stats\n",
        "    correct = (pred==yv).sum().item()\n",
        "    total_correct += correct\n",
        "    total_samples += yv.size(0)\n",
        "    total_loss += loss.item() * yv.size(0)\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_acc = total_correct / total_samples\n",
        "\n",
        "    # TensorBoard logging\n",
        "    writer.add_scalar(\"validate/loss\", avg_loss, epoch)\n",
        "    writer.add_scalar(\"validate/acc\",  avg_acc,  epoch)\n",
        "  print(f\"Validation performed in {(total_time/batch_count)*1000} ms per batch on average\")\n",
        "\n",
        "\n",
        "  y_true = np.concatenate(ground_truth_labels)\n",
        "  y_pred = np.concatenate(predicted_labels)\n",
        "\n",
        "  #Per-class:\n",
        "  labels = np.unique(y_true)\n",
        "  prec, rec, f1, supp = precision_recall_fscore_support(\n",
        "      y_true, y_pred,\n",
        "      beta=1.0,\n",
        "      average=None,\n",
        "      labels=labels,\n",
        "      zero_division=0,\n",
        "  )\n",
        "  #Macro-average instead:\n",
        "  prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "      y_true, y_pred, beta=1.0, average=\"macro\", zero_division=0\n",
        "  )\n",
        "\n",
        "  for label, p, r, f, s in zip(labels, prec, rec, f1, supp):\n",
        "      writer.add_scalar(f\"validate/precision_class_{label}\", p, epoch)\n",
        "      writer.add_scalar(f\"validate/recall_class_{label}\",    r, epoch)\n",
        "      writer.add_scalar(f\"validate/f1_class_{label}\",        f, epoch)\n",
        "      writer.add_scalar(f\"validate/support_class_{label}\",   s, epoch)\n",
        "  writer.add_scalar(\"validate/precision_macro\", prec_macro, epoch)\n",
        "  writer.add_scalar(\"validate/recall_macro\",    rec_macro,  epoch)\n",
        "  writer.add_scalar(\"validate/f1_macro\",        f1_macro,   epoch)\n",
        "\n",
        "  avg_loss = total_loss / total_samples\n",
        "  avg_acc = total_correct / total_samples\n",
        "\n",
        "  print(f\"{\"Validate\":<20}| Epoch {epoch:03d} | loss {avg_loss:.4f} | acc {avg_acc:.4f} | precision {prec_macro:.3f} | f1-score {f1_macro:.3f} |\")\n",
        "  return avg_loss, avg_acc\n",
        "\n",
        "def train_model(\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    N_MELS,\n",
        "    NUM_SPK,\n",
        "    writer,\n",
        "    loss_fn=nn.CrossEntropyLoss(),\n",
        "    epochs=30,\n",
        "    patience = 5,\n",
        "    rnn_hidden=256,\n",
        "    rnn_layers=2,\n",
        "    bidir=True,\n",
        "    use_drive=True, #true if in colab, false if local\n",
        "    checkpoint_dir=\"./checkpoints\",\n",
        "    drive_dir =\"/content/drive/MyDrive/ML_PW/spk_checkpoints\",\n",
        "    optimizer = \"AdamW\",\n",
        "    lr = 1e-3,\n",
        "    weight_decay =1e-4,\n",
        "    T_max = 50\n",
        "    ):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  backbone = Backbone(no_mels=N_MELS, embed_dim=256, rnn_hidden=256, rnn_layers=2, bidir=True)\n",
        "  model = SpeakerClassifier(backbone, num_speakers=NUM_SPK, aamsm_scaler=30.0, aamsm_margin=0.25).to(device)\n",
        "\n",
        "  #optimizers: AdamW, Stochastic Gradient Descent\n",
        "  if optimizer == \"SGD\":\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "  elif optimizer ==\"AdamW\":\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "  #scheduler and scaler\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
        "  scaler = torch.amp.GradScaler(\"cuda\",enabled=torch.cuda.is_available())\n",
        "\n",
        "\n",
        "  #speeds up training if cuda is available, which it should be if training on colab GPU\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "  torch.manual_seed(1337)\n",
        "\n",
        "  global_step = 0\n",
        "  save_root = prepare_save_dir(checkpoint_dir=checkpoint_dir, use_drive=use_drive, drive_dir=drive_dir)\n",
        "  best_val_acc = 0.0\n",
        "  best_path = os.path.join(save_root, \"best_model.pt\")\n",
        "  last_path = os.path.join(save_root, \"last_model.pt\")\n",
        "\n",
        "  for epoch in range(1, epochs+1):\n",
        "    global_step, _, _ = train_one_epoch(model, train_loader, optimizer, scheduler, writer, epoch, device,global_step,loss_fn,scaler)\n",
        "    val_loss, val_acc = validate(model, val_loader, writer, epoch, device, loss_fn)\n",
        "    torch.save(model.state_dict(), last_path)\n",
        "    if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(),best_path)\n",
        "            print(f\"Saved new best model (acc={val_acc:.4f})\")\n",
        "\n",
        "  print(f\"Training complete. Best validation accuracy: {best_val_acc:.4f}\")\n",
        "  return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EluO56RpRVZK"
      },
      "source": [
        "##Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plVOldTDR6AF"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRjBAKEPUfvw"
      },
      "outputs": [],
      "source": [
        "class LMDataset(Dataset):\n",
        "  \"\"\"\n",
        "    Expects layout like:\n",
        "      /train/logmel      -> float (N, T, F) or (N, F, T)\n",
        "      /train/label       -> int64  (N,)           [optional]\n",
        "      /train/length      -> int64  (N,)           [optional, original T]\n",
        "    Equivalent for /val, and /test\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "                h5_path,\n",
        "                split=\"train\",\n",
        "                feature_key=\"logmel\",\n",
        "                label_key=\"label\",\n",
        "                length_key=\"length\",\n",
        "                time_dim=\"F\",   # \"T\" if feature dataset is (N, T, F); \"F\" if (N, F, T)\n",
        "                dtype=np.float32):\n",
        "    super().__init__()\n",
        "    self.h5_path = h5_path\n",
        "    self.split = split\n",
        "    self.feature_key = feature_key\n",
        "    self.label_key = label_key\n",
        "    self.length_key = length_key\n",
        "    self.time_dim = time_dim\n",
        "    self.dtype = dtype\n",
        "\n",
        "\n",
        "    with h5py.File(self.h5_path, \"r\") as f:\n",
        "      grp = f[self.split]\n",
        "      self.N = grp[self.feature_key].shape[0]\n",
        "      self.has_labels = self.label_key in grp\n",
        "      self.has_lengths = self.length_key in grp\n",
        "\n",
        "    self._h5 = None\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "\n",
        "  def _ensure_open(self):\n",
        "      if self._h5 is None:\n",
        "          self._h5 = h5py.File(self.h5_path, \"r\", swmr=True, libver=\"latest\")\n",
        "          self._grp = self._h5[self.split]\n",
        "          self._X = self._grp[self.feature_key]\n",
        "          self._Y = self._grp[self.label_key] if self.has_labels else None\n",
        "          self._L = self._grp[self.length_key] if self.has_lengths else None\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    self._ensure_open()\n",
        "    X = np.array(self._X[idx],dtype = self.dtype)\n",
        "    if X.ndim != 2:\n",
        "      raise ValueError(f\"{self.split}/{self.feature_key} must be 2D per item; got {X.shape}\")\n",
        "\n",
        "    if self.time_dim == \"F\":    # data stored (F,T)\n",
        "      X = X.T                   #Transpose to get (T,F)\n",
        "\n",
        "    X = torch.from_numpy(X[None, ...]) #change dimension for Conv2D to (1,T,F)\n",
        "\n",
        "    y = torch.tensor(self._Y[idx], dtype=torch.long) if self.has_labels else None\n",
        "\n",
        "\n",
        "\n",
        "    t_len = int(self._L[idx]) if self.has_lengths else X.shape[1] # prefer saved length else derive it\n",
        "    t_len = torch.tensor(t_len, dtype=torch.long)\n",
        "\n",
        "    return X, y, t_len\n",
        "\n",
        "\n",
        "  def _pad_collate(self,batch, pad_value=-80.0):\n",
        "    xs, ys, lens = zip(*batch)\n",
        "    B = len(xs)\n",
        "    Tmax = max(int(x.shape[1]) for x in xs)\n",
        "    F = xs[0].shape[2]\n",
        "\n",
        "    X = xs[0].new_full((B, 1, Tmax, F), fill_value=pad_value)\n",
        "    for i, x in enumerate(xs):\n",
        "        T = x.shape[1]\n",
        "        X[i, :, :T, :] = x\n",
        "\n",
        "    y = None\n",
        "    if ys[0] is not None:\n",
        "        y = torch.stack(ys, dim=0)\n",
        "\n",
        "    lengths = torch.stack(lens, dim=0)\n",
        "\n",
        "    return X, y, lengths\n",
        "\n",
        "\n",
        "def build_h5_loaders(\n",
        "    h5_path: str,\n",
        "    splits=(\"train\", \"val\", \"test\"),\n",
        "    feature_key=\"logmel\",\n",
        "    label_key=\"label\",\n",
        "    length_key=\"length\",\n",
        "    time_dim=\"F\",           # \"T\" if (N,T,F); \"F\" if (N,F,T)\n",
        "    batch_sizes=None,\n",
        "    num_workers=8,\n",
        "    pad_value=-80.0,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    shuffle_train=True,\n",
        "    ):\n",
        "  \"\"\"\n",
        "  Returns a dict of {split: DataLoader}. Only creates loaders for existing splits in the file.\n",
        "  \"\"\"\n",
        "  loaders = {}\n",
        "\n",
        "  if batch_sizes is None:\n",
        "    batch_sizes = {s: 32 for s in splits}\n",
        "\n",
        "  with h5py.File(h5_path, \"r\") as f:\n",
        "    available = {k for k in f.keys()}  # top-level groups (e.g., \"train\", \"val\", \"test\")\n",
        "\n",
        "  for split in splits:\n",
        "    if split not in available:\n",
        "      continue\n",
        "\n",
        "    dataset = LMDataset(\n",
        "        h5_path=h5_path,\n",
        "        split=split,\n",
        "        feature_key=feature_key,\n",
        "        label_key=label_key,\n",
        "        length_key=length_key,\n",
        "        time_dim=time_dim,\n",
        "    )\n",
        "\n",
        "    dataset_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_sizes.get(split, 32),\n",
        "        shuffle= (shuffle_train and split == \"train\"),\n",
        "        collate_fn= lambda b: dataset._pad_collate(b,pad_value=pad_value),\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        persistent_workers=persistent_workers if num_workers>0 else False,\n",
        "        )\n",
        "    loaders[split] = dataset_loader\n",
        "\n",
        "  return loaders\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q15yScriBkOi"
      },
      "source": [
        "##Inference Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ6JtQLABnEz"
      },
      "source": [
        "pipeline which takes multisecond voice recording and processes it into a batch of 10 milisecond snippets which are then fed into the model using parallel processing;\n",
        "\n",
        "their average should then be return (where P=1, F=0 so that if half of the inferences return False the average is 0.5).\n",
        "\n",
        "Probably only some snippets should be chosen so that we're not processing 500 samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1NSPvbDhGOz"
      },
      "source": [
        "##Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMCK25AWyAtd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive  # type: ignore\n",
        "\n",
        "# drive.mount(\"/content/drive\",force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dge5kaWLlo_g"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "def get_yaml_meta(h5_path):\n",
        "    \"\"\"Read YAML metadata from /meta/file_description.yaml in HDF5.\"\"\"\n",
        "    with h5py.File(h5_path, \"r\") as f:\n",
        "        raw = f[\"/meta/file_description.yaml\"][()].decode(\"utf-8\")\n",
        "    return yaml.safe_load(raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtN2Le6ihIXN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time, os\n",
        "working_directory = \"/content/drive/MyDrive/ML_PW\"\n",
        "log_dir = os.path.join(\"/content/logs\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "writer = SummaryWriter(log_dir=log_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLtX3onG--_I"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1iuzprnud_y"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/ML_PW/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppx2DOYd-_QI"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "def get_yaml_meta(h5_path):\n",
        "    \"\"\"Read YAML metadata from /meta/file_description.yaml in HDF5.\"\"\"\n",
        "    with h5py.File(h5_path, \"r\") as f:\n",
        "        raw = f[\"/meta/file_description.yaml\"][()].decode(\"utf-8\")\n",
        "    return yaml.safe_load(raw)\n",
        "\n",
        "dataset_adress = \"/content/drive/MyDrive/ML_PW/outputs/logmels_volnorm_silrem_04-12-25.h5\"\n",
        "\n",
        "meta = get_yaml_meta(dataset_adress)\n",
        "no_mels = meta[\"n_mels\"]                        # stored key name\n",
        "no_speakers = meta[\"num_speakers\"]              # stored key name\n",
        "frequency_first = meta[\"frequency\"]  == \"True\" if \"frequency\" in meta else True\n",
        "\n",
        "\n",
        "loaders = build_h5_loaders(\n",
        "    dataset_adress,\n",
        "    splits=(\"train\",\"val\",\"test\"),\n",
        "    feature_key=\"logmel\",\n",
        "    label_key=\"label\",                # set to a missing key if you have no labels\n",
        "    length_key=\"length\",              # optional in file; if absent, we infer from T\n",
        "    time_dim=\"F\",                     # set \"F\" if stored as (N,F,T)\n",
        "    num_workers=2\n",
        ")\n",
        "train_loader = loaders[\"train\"]\n",
        "val_loader   = loaders[\"val\"]\n",
        "\n",
        "\n",
        "model = train_model(train_loader,val_loader,no_mels, no_speakers,writer,use_drive=True,drive_dir=\"/content/drive/MyDrive/spk_checkpoints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCydmCaX5pXT"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/logs \"/content/drive/MyDrive/ML_PW/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFl7JPP28UXV"
      },
      "source": [
        "#Loading model from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecqd7NkVtvub"
      },
      "outputs": [],
      "source": [
        "dataset_adress = \"/content/drive/MyDrive/ML_PW/outputs/logmels_volnorm_silrem_04-11-25.h5\"\n",
        "\n",
        "meta = get_yaml_meta(dataset_adress)\n",
        "no_mels = meta[\"n_mels\"]\n",
        "no_speakers = meta[\"num_speakers\"]\n",
        "backbone = Backbone(no_mels=no_mels,\n",
        "                    embed_dim=256,\n",
        "                    rnn_hidden=256,\n",
        "                    rnn_layers=2,\n",
        "                    bidir=True)\n",
        "\n",
        "model = SpeakerClassifier(backbone,\n",
        "                          num_speakers=no_speakers,\n",
        "                          aamsm_scaler=30.0,\n",
        "                          aamsm_margin=0.25)\n",
        "\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/ML_PW/spk_checkpoints/best_model.pt\",\n",
        "                        map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sCiMG1E8s6S"
      },
      "outputs": [],
      "source": [
        "model.test(loaders[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_recording_path = \"/content/test-recording-michal.m4a\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "results = []\n",
        "X, lengths = logmels_to_batch(process_audio_to_logmels(test_recording_path),device)\n",
        "for i in range(X.size(0)): # per-item verify\n",
        "  x_single = X[i:i+1] # keep batch dim [1,1,T,F]\n",
        "  l_single = lengths[i:i+1] # keep batch dim [1]\n",
        "  pred_ids, scores, decisions = model.infer( x=x_single, lengths=l_single, threshold=0.7 )\n",
        "  results.append(( int(pred_ids.item()), # True/False\n",
        "                  float(scores.item()),)) # similarity\n",
        "\n",
        "for res in results:\n",
        "  print(f\"predicted id: {res[0]} | similarity: {res[1]}\")"
      ],
      "metadata": {
        "id": "Pr1S0DIA3QXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "562eacee"
      },
      "source": [
        "### Export Model to ONNX\n",
        "\n",
        "This section exports the `backbone` of the trained `SpeakerClassifier` model to ONNX format. The backbone is responsible for generating speaker embeddings, which is typically what you need for inference tasks like speaker verification or identification.\n",
        "\n",
        "The export process requires a dummy input tensor that matches the expected shape of the model's input (log-mel spectrograms). We also specify `dynamic_axes` to allow for variable batch sizes and time steps in the ONNX model, making it more flexible for inference with different input lengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d335ea9"
      },
      "source": [
        "import torch.onnx\n",
        "\n",
        "def export_to_onnx(model, onnx_path, n_mels, dummy_time_steps=32):\n",
        "    \"\"\"\n",
        "    Exports the model's backbone to ONNX format.\n",
        "\n",
        "    Args:\n",
        "        model (SpeakerClassifier): The trained PyTorch SpeakerClassifier model.\n",
        "        onnx_path (str): The path where the ONNX model will be saved.\n",
        "        n_mels (int): The number of mel bands used for spectrograms.\n",
        "        dummy_time_steps (int): A representative number of time steps for the dummy input.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create a dummy input for tracing\n",
        "    # The input to the backbone is [B, 1, T, F]\n",
        "    # Let's use a batch size of 1, 1 channel, dummy_time_steps, and n_mels features\n",
        "    dummy_input_x = torch.randn(1, 1, dummy_time_steps, n_mels).to(model.device)\n",
        "    dummy_lengths = torch.tensor([dummy_time_steps], dtype=torch.long).to(model.device)\n",
        "\n",
        "    # Define input and output names for the ONNX graph\n",
        "    input_names = [\"input_features\", \"input_lengths\"]\n",
        "    output_names = [\"output_embedding\"]\n",
        "\n",
        "    # Define dynamic axes for variable batch size and time steps\n",
        "    dynamic_axes = {\n",
        "        \"input_features\": {0: \"batch_size\", 2: \"time_steps\"},\n",
        "        \"input_lengths\": {0: \"batch_size\"},\n",
        "        \"output_embedding\": {0: \"batch_size\"},\n",
        "    }\n",
        "\n",
        "    print(f\"Exporting model to ONNX at: {onnx_path}\")\n",
        "    torch.onnx.export(\n",
        "        model.backbone,  # Export only the backbone\n",
        "        (dummy_input_x, dummy_lengths),  # Model input\n",
        "        onnx_path,       # Output file name\n",
        "        verbose=False,\n",
        "        input_names=input_names,\n",
        "        output_names=output_names,\n",
        "        dynamic_axes=dynamic_axes,\n",
        "        opset_version=11 # Opset version, ensure compatibility\n",
        "    )\n",
        "    print(\"ONNX export complete!\")\n",
        "\n",
        "# Define the output path for the ONNX model\n",
        "onnx_output_path = \"/content/drive/MyDrive/ML_PW/speaker_classifier_backbone.onnx\"\n",
        "\n",
        "# Execute the export function\n",
        "export_to_onnx(model, onnx_output_path, no_mels)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}