{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95cc65e",
   "metadata": {},
   "source": [
    "# ğŸ“¦ ONNX Export Notebook\n",
    "\n",
    "This notebook exports the trained speaker verification model to **ONNX format** for deployment.\n",
    "\n",
    "## ğŸ”„ What is ONNX?\n",
    "\n",
    "**ONNX** (Open Neural Network Exchange) is an open format for representing machine learning models. It allows you to:\n",
    "\n",
    "1. **Deploy to different platforms**: Run the model in C++, C#, Java, JavaScript, etc.\n",
    "2. **Use different runtimes**: ONNX Runtime, TensorRT, OpenVINO for optimization\n",
    "3. **Cross-framework compatibility**: Export from PyTorch, use in TensorFlow or other frameworks\n",
    "4. **Optimize for inference**: Apply quantization, graph optimizations\n",
    "\n",
    "## ğŸ“Š Export Strategy\n",
    "\n",
    "We export only the **Backbone** (CNN + RNN) which produces speaker embeddings:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    FULL MODEL (Training)                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚\n",
    "â”‚  â”‚   Backbone   â”‚ â†’ â”‚  Embeddings  â”‚ â†’ â”‚     AAMSoftmax Head      â”‚â”‚\n",
    "â”‚  â”‚  (CNN+RNN)   â”‚   â”‚   [B, 256]   â”‚   â”‚  (Only for training)     â”‚â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    EXPORTED MODEL (ONNX)                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\n",
    "â”‚  â”‚   Backbone   â”‚ â†’ â”‚  Embeddings  â”‚  â† This is what we export    â”‚\n",
    "â”‚  â”‚  (CNN+RNN)   â”‚   â”‚   [B, 256]   â”‚                               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Why only the backbone?**\n",
    "- The AAMSoftmax head is only used during training to learn better embeddings\n",
    "- At inference time, we compare embeddings directly using cosine similarity\n",
    "- The speaker bank (reference embeddings) is stored separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5976dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import h5py\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.onnx\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“¦ ONNX EXPORT SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT CHECKPOINT AND OUTPUT PATH\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“ AVAILABLE CHECKPOINTS:\")\n",
    "checkpoint_dirs = sorted(glob.glob(\"checkpoints/train*\"), key=os.path.getmtime, reverse=True)\n",
    "for i, cp in enumerate(checkpoint_dirs[:10]):\n",
    "    best_path = os.path.join(cp, \"best_model.pt\")\n",
    "    exists = \"âœ“\" if os.path.exists(best_path) else \"âœ—\"\n",
    "    mtime = datetime.fromtimestamp(os.path.getmtime(cp)).strftime(\"%Y-%m-%d %H:%M\")\n",
    "    print(f\"   {i+1}. {cp} [{exists}] ({mtime})\")\n",
    "\n",
    "# âš ï¸ CONFIGURE THESE PATHS\n",
    "checkpoint_path = os.path.join(checkpoint_dirs[0], \"best_model.pt\") if checkpoint_dirs else \"checkpoints/train1/best_model.pt\"\n",
    "onnx_output_path = \"./checkpoints/speaker_backbone.onnx\"\n",
    "\n",
    "# Dataset path for metadata\n",
    "available_datasets = sorted(glob.glob(\"outputs/logmels_*.h5\"), key=os.path.getmtime, reverse=True)\n",
    "dataset_path = available_datasets[0] if available_datasets else \"outputs/logmels_aug.h5\"\n",
    "\n",
    "print(f\"\\nâœ… Selected Checkpoint: {checkpoint_path}\")\n",
    "print(f\"âœ… ONNX Output: {onnx_output_path}\")\n",
    "print(f\"âœ… Dataset (for metadata): {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba94d4",
   "metadata": {},
   "source": [
    "## Section 2: Model Architecture\n",
    "\n",
    "Define the model architecture (must match training exactly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed04768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: MODEL ARCHITECTURE (Same as training)\n",
    "# ============================================================================\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block.\"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels // reduction, 4)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, hidden, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, F = x.shape\n",
    "        s = x.mean(dim=(2, 3))\n",
    "        w = self.fc(s).view(B, C, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    \"\"\"CNN + RNN backbone for speaker embeddings.\"\"\"\n",
    "    def __init__(self, no_mels, embed_dim, rnn_hidden, rnn_layers, bidir):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            SEBlock(32, reduction=8),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2)),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            SEBlock(64, reduction=8),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2)),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            SEBlock(128, reduction=8),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2)),\n",
    "        )\n",
    "\n",
    "        self.rnn_hidden = rnn_hidden\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=128 * (no_mels // 8),\n",
    "            hidden_size=self.rnn_hidden,\n",
    "            num_layers=rnn_layers,\n",
    "            bidirectional=bidir,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        out_dim = (2 if bidir else 1) * rnn_hidden\n",
    "        self.rnn_ln = nn.LayerNorm(out_dim)\n",
    "\n",
    "        self.att = nn.Sequential(\n",
    "            nn.Linear((2 if bidir else 1)*rnn_hidden, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(out_dim*2, 256),\n",
    "            nn.BatchNorm1d(256), nn.ReLU(),\n",
    "            nn.Linear(256, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths=None, mc_dropout=None):\n",
    "        if mc_dropout is None:\n",
    "            mc_dropout = self.training\n",
    "            \n",
    "        h = self.cnn_block(x)\n",
    "        if mc_dropout:\n",
    "            h = F.dropout(h, p=0.3, training=True)\n",
    "            \n",
    "        B, C, T, Fp = h.shape\n",
    "        h = h.permute(0, 2, 1, 3).contiguous().view(B, T, C*Fp)\n",
    "\n",
    "        if lengths is not None:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(h, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            packed_out, _ = self.rnn(packed)\n",
    "            rnn_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "            Tmax = rnn_out.size(1)\n",
    "            mask = torch.arange(Tmax, device=rnn_out.device).unsqueeze(0).expand(B, Tmax) < lengths.unsqueeze(1)\n",
    "        else:\n",
    "            rnn_out, _ = self.rnn(h)\n",
    "            mask = torch.ones(rnn_out.size(0), rnn_out.size(1), dtype=torch.bool, device=rnn_out.device)\n",
    "\n",
    "        if mc_dropout:\n",
    "            rnn_out = F.dropout(rnn_out, p=0.3, training=True)\n",
    "\n",
    "        rnn_out = self.rnn_ln(rnn_out)\n",
    "\n",
    "        a = self.att(rnn_out).squeeze(-1)\n",
    "        a = a.masked_fill(~mask, float('-inf'))\n",
    "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
    "\n",
    "        mean = torch.sum(w * rnn_out, dim=1)\n",
    "        var = torch.sum(w * (rnn_out - mean.unsqueeze(1))**2, dim=1)\n",
    "        std = torch.sqrt(var + 1e-5)\n",
    "        stats = torch.cat([mean, std], 1)\n",
    "\n",
    "        if mc_dropout:\n",
    "            stats = F.dropout(stats, p=0.3, training=True)\n",
    "\n",
    "        z = self.proj(stats)\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class AAMSoftmax(nn.Module):\n",
    "    \"\"\"AAMSoftmax head (needed for loading checkpoint).\"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.20):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, emb, labels):\n",
    "        W = F.normalize(self.weight, dim=1)\n",
    "        cos_theta = emb @ W.T\n",
    "        theta = torch.acos(cos_theta.clamp(-1+1e-7, 1-1e-7))\n",
    "        target_logits = torch.cos(theta + self.m)\n",
    "        one_hot = F.one_hot(labels, num_classes=W.size(0)).float()\n",
    "        output = cos_theta * (1 - one_hot) + target_logits * one_hot\n",
    "        return output * self.s\n",
    "\n",
    "\n",
    "class SpeakerClassifier(nn.Module):\n",
    "    \"\"\"Full model (loads checkpoint, but we only export backbone).\"\"\"\n",
    "    def __init__(self, backbone, num_speakers, aamsm_scaler, aamsm_margin):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.aamsm = AAMSoftmax(backbone.proj[-1].out_features, num_speakers, aamsm_scaler, aamsm_margin)\n",
    "        self._inference_prepared = False\n",
    "        self.score_alpha = nn.Parameter(torch.tensor(1.0))\n",
    "        self.score_beta = nn.Parameter(torch.tensor(0.0))\n",
    "        self.bank = None\n",
    "        self.inference_threshold = 0.5\n",
    "\n",
    "    def forward(self, x, labels=None, lengths=None):\n",
    "        emb = self.backbone(x, lengths=lengths, mc_dropout=None)\n",
    "        if labels is not None:\n",
    "            logits = self.aamsm(emb, labels)\n",
    "            return logits, emb\n",
    "        return emb\n",
    "\n",
    "\n",
    "print(\"âœ… Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623f2de",
   "metadata": {},
   "source": [
    "## Section 3: Load Model and Checkpoint\n",
    "\n",
    "Load the trained model weights from the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dec1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: LOAD MODEL AND CHECKPOINT\n",
    "# ============================================================================\n",
    "\n",
    "def get_yaml_meta(h5_path):\n",
    "    \"\"\"Read metadata from HDF5.\"\"\"\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        raw = f[\"/meta/file_description.yaml\"][()].decode(\"utf-8\")\n",
    "        meta_yaml = yaml.safe_load(raw) or {}\n",
    "        meta_grp = f[\"/meta\"]\n",
    "        attrs = {k: (int(v) if isinstance(v, (np.integer,)) else v) for k, v in meta_grp.attrs.items()}\n",
    "        meta_yaml.update(attrs)\n",
    "        if \"speaker_mapping.yaml\" in meta_grp:\n",
    "            sm_raw = meta_grp[\"speaker_mapping.yaml\"][()].decode(\"utf-8\")\n",
    "            sm = yaml.safe_load(sm_raw) or {}\n",
    "            meta_yaml[\"total_speakers\"] = len(sm.get(\"speakers\", {}))\n",
    "    return meta_yaml\n",
    "\n",
    "\n",
    "# Load metadata to get model dimensions\n",
    "meta = get_yaml_meta(dataset_path)\n",
    "no_mels = meta[\"preprocessing_config\"][\"n_mels\"]\n",
    "num_classes = meta.get(\"num_classes\", meta[\"total_speakers\"])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š MODEL CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Mel Bands: {no_mels}\")\n",
    "print(f\"   Num Classes: {num_classes}\")\n",
    "\n",
    "# Create model\n",
    "backbone = Backbone(\n",
    "    no_mels=no_mels, \n",
    "    embed_dim=256, \n",
    "    rnn_hidden=256, \n",
    "    rnn_layers=2, \n",
    "    bidir=True\n",
    ")\n",
    "model = SpeakerClassifier(\n",
    "    backbone, \n",
    "    num_speakers=num_classes, \n",
    "    aamsm_scaler=30.0, \n",
    "    aamsm_margin=0.25\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "print(f\"\\nğŸ“¥ Loading checkpoint: {checkpoint_path}\")\n",
    "try:\n",
    "    state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"âœ… Checkpoint loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: Checkpoint not found!\")\n",
    "    print(\"   Please update 'checkpoint_path' above.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"âŒ ERROR: {e}\")\n",
    "\n",
    "model.eval()\n",
    "print(\"âœ… Model set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b64d5",
   "metadata": {},
   "source": [
    "## Section 4: Export to ONNX\n",
    "\n",
    "The export process:\n",
    "1. **Move model to CPU** - Standard practice for ONNX export\n",
    "2. **Create dummy inputs** - ONNX uses tracing, so it needs example inputs\n",
    "3. **Define dynamic axes** - Allow variable batch size and sequence length\n",
    "4. **Export backbone only** - The AAMSoftmax head is not needed at inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: EXPORT TO ONNX\n",
    "# ============================================================================\n",
    "\n",
    "def export_to_onnx(model, onnx_path, n_mels, dummy_time_steps=94):\n",
    "    \"\"\"\n",
    "    Export the model's backbone to ONNX format.\n",
    "\n",
    "    Args:\n",
    "        model: SpeakerClassifier with trained weights\n",
    "        onnx_path: Output path for ONNX file\n",
    "        n_mels: Number of mel bands (must match training)\n",
    "        dummy_time_steps: Representative time steps (94 â‰ˆ 3 seconds at default settings)\n",
    "    \n",
    "    Notes:\n",
    "        - We export only the BACKBONE (not AAMSoftmax)\n",
    "        - Dynamic axes allow variable batch sizes and sequence lengths\n",
    "        - The model outputs L2-normalized embeddings of shape [B, 256]\n",
    "    \"\"\"\n",
    "    # Move to CPU for export\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ“¦ ONNX EXPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create dummy inputs\n",
    "    # Input shape: [Batch, Channel=1, Time, Frequency=n_mels]\n",
    "    dummy_input_x = torch.randn(1, 1, dummy_time_steps, n_mels, device=device)\n",
    "    dummy_lengths = torch.tensor([dummy_time_steps], dtype=torch.long, device=device)\n",
    "\n",
    "    print(f\"   Dummy input shape: {dummy_input_x.shape}\")\n",
    "    print(f\"   Dummy lengths: {dummy_lengths}\")\n",
    "\n",
    "    # Define input/output names\n",
    "    input_names = [\"input_spectrogram\", \"input_lengths\"]\n",
    "    output_names = [\"embedding\"]\n",
    "\n",
    "    # Dynamic axes for flexibility\n",
    "    dynamic_axes = {\n",
    "        \"input_spectrogram\": {\n",
    "            0: \"batch_size\",     # Variable batch\n",
    "            2: \"time_steps\"      # Variable sequence length\n",
    "        },\n",
    "        \"input_lengths\": {\n",
    "            0: \"batch_size\"\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            0: \"batch_size\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f\"\\n   Exporting to: {onnx_path}\")\n",
    "    \n",
    "    # Export\n",
    "    torch.onnx.export(\n",
    "        model.backbone,                    # Only the backbone!\n",
    "        (dummy_input_x, dummy_lengths),    # Example inputs\n",
    "        onnx_path,\n",
    "        verbose=False,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=14,                  # Use opset 14 for better compatibility\n",
    "        do_constant_folding=True,          # Optimize constants\n",
    "    )\n",
    "    \n",
    "    # Verify file was created\n",
    "    if os.path.exists(onnx_path):\n",
    "        file_size = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… ONNX export successful!\")\n",
    "        print(f\"   File: {onnx_path}\")\n",
    "        print(f\"   Size: {file_size:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Export failed - file not created\")\n",
    "    \n",
    "    return onnx_path\n",
    "\n",
    "\n",
    "# Run export\n",
    "exported_path = export_to_onnx(model, onnx_output_path, no_mels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c37ee",
   "metadata": {},
   "source": [
    "## Section 5: Verify ONNX Model\n",
    "\n",
    "Validate the exported ONNX model by running a test inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: VERIFY ONNX MODEL\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ” VERIFYING ONNX MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load and check the ONNX model\n",
    "    onnx_model = onnx.load(onnx_output_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"âœ… ONNX model structure is valid\")\n",
    "    \n",
    "    # Print model inputs/outputs\n",
    "    print(\"\\nğŸ“¥ Model Inputs:\")\n",
    "    for inp in onnx_model.graph.input:\n",
    "        shape = [dim.dim_value if dim.dim_value else dim.dim_param \n",
    "                 for dim in inp.type.tensor_type.shape.dim]\n",
    "        print(f\"   â€¢ {inp.name}: {shape}\")\n",
    "    \n",
    "    print(\"\\nğŸ“¤ Model Outputs:\")\n",
    "    for out in onnx_model.graph.output:\n",
    "        shape = [dim.dim_value if dim.dim_value else dim.dim_param \n",
    "                 for dim in out.type.tensor_type.shape.dim]\n",
    "        print(f\"   â€¢ {out.name}: {shape}\")\n",
    "    \n",
    "    # Test inference with ONNX Runtime\n",
    "    print(\"\\nğŸ§ª Testing inference with ONNX Runtime...\")\n",
    "    session = ort.InferenceSession(onnx_output_path)\n",
    "    \n",
    "    # Create test input\n",
    "    test_input = np.random.randn(1, 1, 94, no_mels).astype(np.float32)\n",
    "    test_lengths = np.array([94], dtype=np.int64)\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = session.run(\n",
    "        None, \n",
    "        {\n",
    "            \"input_spectrogram\": test_input,\n",
    "            \"input_lengths\": test_lengths\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    embedding = outputs[0]\n",
    "    print(f\"   Input shape: {test_input.shape}\")\n",
    "    print(f\"   Output shape: {embedding.shape}\")\n",
    "    print(f\"   Embedding norm: {np.linalg.norm(embedding):.4f} (should be ~1.0)\")\n",
    "    \n",
    "    # Compare with PyTorch output\n",
    "    print(\"\\nğŸ”„ Comparing with PyTorch output...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        torch_input = torch.from_numpy(test_input)\n",
    "        torch_lengths = torch.from_numpy(test_lengths)\n",
    "        torch_output = model.backbone(torch_input, torch_lengths).numpy()\n",
    "    \n",
    "    diff = np.abs(embedding - torch_output).max()\n",
    "    print(f\"   Max difference: {diff:.6f}\")\n",
    "    \n",
    "    if diff < 1e-4:\n",
    "        print(\"   âœ… Outputs match! ONNX export is correct.\")\n",
    "    else:\n",
    "        print(\"   âš ï¸ Small differences detected (may be due to floating point precision)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… ONNX VERIFICATION COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Install onnx and onnxruntime for verification:\")\n",
    "    print(\"   pip install onnx onnxruntime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cbe25",
   "metadata": {},
   "source": [
    "## Section 6: Export Speaker Bank\n",
    "\n",
    "For inference, you also need the speaker reference embeddings (bank). Export them alongside the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f853f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: EXPORT SPEAKER BANK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“¦ EXPORTING SPEAKER BANK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the speaker bank from AAMSoftmax weights\n",
    "model.eval()\n",
    "speaker_bank = F.normalize(model.aamsm.weight, dim=1).detach().cpu().numpy()\n",
    "\n",
    "label_mode = meta.get(\"label_mode\", \"speaker\")\n",
    "print(f\"   Label mode: {label_mode}\")\n",
    "print(f\"   Speaker bank shape: {speaker_bank.shape}\")\n",
    "\n",
    "if label_mode == \"binary\":\n",
    "    print(\"   â†’ Binary mode: class 0 = outsider, class 1 = group_member\")\n",
    "else:\n",
    "    print(\"   â†’ Speaker ID mode: each row is a speaker embedding\")\n",
    "\n",
    "# Save as numpy file\n",
    "bank_path = onnx_output_path.replace(\".onnx\", \"_speaker_bank.npy\")\n",
    "np.save(bank_path, speaker_bank)\n",
    "print(f\"\\nâœ… Speaker bank saved to: {bank_path}\")\n",
    "\n",
    "# Also save speaker mapping if available\n",
    "export_meta = {}\n",
    "if \"speaker_mapping\" in meta:\n",
    "    export_meta[\"speaker_mapping\"] = meta[\"speaker_mapping\"]\n",
    "export_meta[\"label_mode\"] = label_mode\n",
    "export_meta[\"num_classes\"] = speaker_bank.shape[0]\n",
    "export_meta[\"embed_dim\"] = speaker_bank.shape[1]\n",
    "\n",
    "mapping_path = onnx_output_path.replace(\".onnx\", \"_metadata.yaml\")\n",
    "with open(mapping_path, \"w\") as f:\n",
    "    yaml.dump(export_meta, f, default_flow_style=False)\n",
    "print(f\"âœ… Metadata saved to: {mapping_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ EXPORTED FILES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   1. ONNX Model:   {onnx_output_path}\")\n",
    "print(f\"   2. Speaker Bank: {bank_path}\")\n",
    "print(f\"   3. Metadata:     {mapping_path}\")\n",
    "print(f\"\\n   Mode: {'Binary (member/outsider)' if label_mode == 'binary' else 'Speaker ID (multi-class)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74589d0",
   "metadata": {},
   "source": [
    "## Section 7: How to Use the ONNX Model\n",
    "\n",
    "Example code for using the exported ONNX model in different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddec4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "# This is a complete example of how to use the exported ONNX model\n",
    "\n",
    "usage_code = '''\n",
    "# ============================================================================\n",
    "# PYTHON USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import librosa\n",
    "\n",
    "# 1. Load the ONNX model and speaker bank\n",
    "session = ort.InferenceSession(\"speaker_backbone.onnx\")\n",
    "speaker_bank = np.load(\"speaker_backbone_speaker_bank.npy\")\n",
    "\n",
    "# 2. Preprocess audio to log-mel spectrogram\n",
    "def preprocess_audio(audio_path, sr=16000, n_mels=64, n_fft=2048, hop_length=512):\n",
    "    \"\"\"Convert audio file to log-mel spectrogram.\"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=sr, mono=True)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "    logmel = librosa.power_to_db(mel, ref=np.max)\n",
    "    \n",
    "    # Shape: [1, 1, T, F]\n",
    "    logmel = logmel.T[np.newaxis, np.newaxis, :, :]\n",
    "    return logmel.astype(np.float32)\n",
    "\n",
    "# 3. Run inference\n",
    "def verify_speaker(audio_path, threshold=0.7):\n",
    "    \"\"\"Verify if speaker is authorized.\"\"\"\n",
    "    # Preprocess\n",
    "    spectrogram = preprocess_audio(audio_path)\n",
    "    lengths = np.array([spectrogram.shape[2]], dtype=np.int64)\n",
    "    \n",
    "    # Get embedding\n",
    "    outputs = session.run(None, {\n",
    "        \"input_spectrogram\": spectrogram,\n",
    "        \"input_lengths\": lengths\n",
    "    })\n",
    "    embedding = outputs[0]\n",
    "    \n",
    "    # Compare to speaker bank (cosine similarity)\n",
    "    similarities = embedding @ speaker_bank.T\n",
    "    best_idx = np.argmax(similarities)\n",
    "    best_score = similarities[0, best_idx]\n",
    "    \n",
    "    # Decision\n",
    "    is_authorized = best_score >= threshold\n",
    "    \n",
    "    return {\n",
    "        \"speaker_id\": int(best_idx),\n",
    "        \"confidence\": float(best_score),\n",
    "        \"authorized\": bool(is_authorized)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = verify_speaker(\"test_audio.wav\")\n",
    "print(f\"Speaker ID: {result['speaker_id']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Access: {'GRANTED' if result['authorized'] else 'DENIED'}\")\n",
    "'''\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“– USAGE EXAMPLE (Python with ONNX Runtime)\")\n",
    "print(\"=\"*70)\n",
    "print(usage_code)\n",
    "\n",
    "# Save usage example to file\n",
    "usage_path = onnx_output_path.replace(\".onnx\", \"_usage_example.py\")\n",
    "with open(usage_path, \"w\") as f:\n",
    "    f.write(usage_code)\n",
    "print(f\"\\nâœ… Usage example saved to: {usage_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b672e7",
   "metadata": {},
   "source": [
    "## ğŸ“š Summary\n",
    "\n",
    "### Files Exported:\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `speaker_backbone.onnx` | The neural network model (backbone only) |\n",
    "| `speaker_backbone_speaker_bank.npy` | Reference embeddings for each speaker |\n",
    "| `speaker_backbone_speaker_mapping.yaml` | Speaker ID to name mapping |\n",
    "| `speaker_backbone_usage_example.py` | Python example code |\n",
    "\n",
    "### Inference Pipeline:\n",
    "1. **Preprocess**: Audio â†’ Log-mel spectrogram\n",
    "2. **Embed**: Spectrogram â†’ ONNX model â†’ 256-dim embedding\n",
    "3. **Match**: Cosine similarity with speaker bank\n",
    "4. **Decide**: Compare max similarity to threshold\n",
    "\n",
    "### Deployment Options:\n",
    "- **Python**: `onnxruntime` (CPU or GPU)\n",
    "- **C++**: ONNX Runtime C++ API\n",
    "- **JavaScript**: `onnxruntime-web` for browser\n",
    "- **Mobile**: ONNX Runtime for iOS/Android\n",
    "- **Edge**: TensorRT, OpenVINO for optimization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
